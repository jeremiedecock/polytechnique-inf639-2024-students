{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Policy-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2024](https://moodle.polytechnique.fr/course/view.php?id=19358) Lab session #2\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab2_deep_policy-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab2_deep_policy-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab2_deep_policy-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab2_deep_policy-based_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of policy-based reinforcement learning techniques, with a particular focus on the *Monte Carlo Policy Gradient (REINFORCE)* and *Actor Critic* methods.\n",
    "\n",
    "In this Python notebook, you'll have the opportunity to implement and assess several renowned Policy Gradient techniques.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab2_deep_policy-based_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab2_deep_policy-based_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab2_deep_policy-based_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `torch`, `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the provided [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) and [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) files.\n",
    "\n",
    "- [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains the minimal dependencies required to run this notebook without the optional sections.\n",
    "- [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains all the dependencies required to run this notebook with all the optional sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-lab2-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RJfFvm-4mOI2"
   },
   "outputs": [],
   "source": [
    "#! apt install xvfb x11-utils && pip install gymnasium pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, first download the [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) or [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) depending on whether you want to run the optional sections of this notebook or not (c.f. *Setup the Python environment* section above).\n",
    "\n",
    "Ensure it is located in the same directory as this notebook. Next, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version.\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "env\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work jdhp/inf639-lab2:latest\n",
    "```\n",
    "\n",
    "If you encounter an error during the notebook's execution indicating that writing a file is not possible, this issue may stem from the user ID within the container lacking the necessary permissions in the project directory. This problem can be resolved by modifying the directory's permissions, for example, using the command:\n",
    "\n",
    "```bash\n",
    "chmod 777 . figs models\n",
    "rm -rf figs/*.gif\n",
    "rm -rf figs/*.png\n",
    "rm -rf models/*.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from dataclasses import dataclass\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "from IPython.display import Video\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    # from tensorboardX import SummaryWriter\n",
    "    warnings.warn(\"tensorboard is not installed\")\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    # wandb.login()\n",
    "except:\n",
    "    warnings.warn(\"Wandb is not installed\")\n",
    "# from inf639 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "PLOTS_DIR = Path(\"figs/\")      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\")   # Where to save models (.pth files)\n",
    "LOGS_DIR = Path(\"logs/\")       # Where to save logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir()\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir()\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir()\n",
    "if not LOGS_DIR.exists():\n",
    "    LOGS_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to visualize the episodes as animated GIFs. Run the cell below to enable this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\", fps=29):\n",
    "        if is_colab() or self.force_gif:\n",
    "            gif_path = filename.with_suffix('.gif')\n",
    "            imageio.mimsave(gif_path, [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=fps, loop=0)\n",
    "            return Image(open(gif_path,'rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `DEFAULT_NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed. \n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined (the `NUMBER_OF_TRAININGS` variable is commented out at the beginning of these cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU.\n",
    "\n",
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful for CartPole (but useful for MuJoCo) because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a recent GPU (e.g. RTX 4060 Ti 16G) and want to use it, you may need to install a specific version of PyTorch compatible with your Cuda version (e.g. Cuda 12.4). For this, you will have to edit the `requirements.txt` file and replace the current version of PyTorch with the one compatible with your Cuda version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Monte Carlo Policy Gradient (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Policy Gradient theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "We will solve the CartPole environment using a policy gradient method which directly searchs in a family of parameterized policies $\\pi_\\theta$ for the optimal policy.\n",
    "\n",
    "This method performs gradient ascent in the policy space so that the total return is maximized.\n",
    "We will restrict our work to episodic tasks, *i.e.* tasks that have a starting states and last for a finite and fixed number of steps $T$, called horizon. \n",
    "\n",
    "More formally, we define an optimization criterion that we want to maximize:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\left[\\sum_{t=1}^T r(s_t,a_t)\\right],$$\n",
    "\n",
    "where $\\mathbb{E}_{\\pi_\\theta}$ means $a \\sim \\pi_\\theta(\\cdot|s)$ and $T$ is the horizon of the episode.\n",
    "In other words, we want to maximize the value of the starting state: $V^{\\pi_\\theta}(s)$.\n",
    "The policy gradient theorem tells us that:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^{\\pi_\\theta}(s) = \\mathbb{E}_{\\pi_\\theta} \\left[\\nabla_\\theta \\log \\pi_\\theta (a|s) ~ Q^{\\pi_\\theta}(s,a) \\right],\n",
    "$$\n",
    "\n",
    "where the $Q$-function is defined as:\n",
    "\n",
    "$$Q^{\\pi_\\theta}(a|s) = \\mathbb{E}^{\\pi_\\theta} \\left[\\sum_{t=1}^T r(s_t,a_t)|s=s_1, a=a_1\\right].$$\n",
    "\n",
    "The policy gradient theorem is particularly effective because it allows gradient computation without needing to understand the system's dynamics, as long as the $Q$-function for the current policy is computable. By simply applying the policy and observing the one-step transitions, sufficient information is gathered. Implementing a stochastic gradient ascent and substituting $Q^{\\pi_\\theta}(s_t,a_t)$ with a Monte Carlo estimate $R_t = \\sum_{t'=t}^T r(s_{t'},a_{t'})$ for a single trajectory, we derive the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The REINFORCE algorithm, introduced by Williams in 1992, is a Monte Carlo policy gradient method. It updates the policy in the direction that maximizes rewards, using full-episode returns as an unbiased estimate of the gradient. Each step involves generating an episode using the current policy, computing the gradient estimate, and updating the policy parameters. This algorithm is simple yet powerful, and it's particularly effective in environments where the policy gradient is noisy or the dynamics are complex.\n",
    "\n",
    "For further reading and a deeper understanding, refer to Williams' seminal paper (https://link.springer.com/article/10.1007/BF00992696) and the comprehensive text on reinforcement learning by Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.13 (http://incompleteideas.net/book/RLbook2020.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the REINFORCE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo policy gradient (REINFORCE)\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    "$\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ A learning rate $\\alpha \\in \\mathbb{R}^+$ <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    "$\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    "$\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    "$\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "$\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "$\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha ~ \\underbrace{G ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)}_{\\nabla_{\\boldsymbol{\\theta}} J(\\boldsymbol{\\theta})}$ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: REINFORCE for discrete action spaces (Cartpole)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue to work on the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "\n",
    "Reminder:\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 200 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy Implementation\n",
    "\n",
    "We will implement a stochastic policy to control the cart using a simple one-layer neural network. Given the simplicity of the problem, a single layer will suffice. We will not incorporate a bias term in this layer.\n",
    "\n",
    "This neural network will output the probabilities of each possible action (in this case, there are only two actions: \"push left\" or \"push right\") given the input vector $s$ (the 4-dimensional state vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.1**: Implement the `PolicyNetwork`  defined as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network takes an input tensor representing the state of the environment and outputs a tensor of action probabilities.\n",
    "The network has the following components:\n",
    "\n",
    "- `layer1`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs `n_actions`. It does not include a bias term.\n",
    "\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor of action probabilities. It first applies the linear layer to the input state tensor to get the logits (the raw, unnormalized scores for each action), and then applies the softmax function to the logits to get the action probabilities. The softmax function ensures that the action probabilities are positive and sum to 1, so they can be interpreted as probabilities.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state or action spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network used as a policy for the REINFORCE algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        A fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(state: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the PolicyNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of PolicyNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        self.layer1 = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "\n",
    "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculate the probability of each action for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor (state).\n",
    "            The shape of the tensor should be (N, dim),\n",
    "            where N is the number of states vectors in the batch\n",
    "            and dim is the dimension of state vectors.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (the probability of each action for the given state).\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        logits = ...\n",
    "        out = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.2**: Complete the `sample_discrete_action` function. This function is used to sample a discrete action based on a given state and a policy network. It first converts the state into a tensor and passes it through the policy network to get the parameters of the action probability distribution. Then, it creates a categorical distribution from these parameters and samples an action from this distribution. It also calculates the log probability of the sampled action according to the distribution. The function returns the sampled action and its log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_discrete_action(policy_nn: PolicyNetwork,\n",
    "                           state: NDArray[np.float64]) -> Tuple[int, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Sample a discrete action based on the given state and policy network.\n",
    "\n",
    "    This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
    "    The action is sampled from a categorical distribution defined by the output of the policy network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy network that defines the probability distribution of the actions.\n",
    "    state : NDArray[np.float64]\n",
    "        The state based on which an action needs to be sampled.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[int, torch.Tensor]\n",
    "        The sampled action and its log probability.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    # Convert the state into a tensor, specify its data type as float32, and send it to the device (CPU or GPU).\n",
    "    # The unsqueeze(0) function is used to add an extra dimension to the tensor to match the input shape required by the policy network.\n",
    "    state_tensor = ...\n",
    "\n",
    "    # Pass the state tensor through the policy network to get the parameters of the action probability distribution.\n",
    "    actions_probability_distribution_params = ...\n",
    "\n",
    "    # Create the categorical distribution used to sample an action from the parameters obtained from the policy network.\n",
    "    # See https://pytorch.org/docs/stable/distributions.html#categorical\n",
    "    actions_probability_distribution = ...\n",
    "\n",
    "    # Sample an action from the categorical distribution.\n",
    "    sampled_action_tensor = ...\n",
    "\n",
    "    # Convert the tensor containing the sampled action into a Python integer.\n",
    "    sampled_action = ...\n",
    "\n",
    "    # Calculate the log probability of the sampled action according to the categorical distribution.\n",
    "    sampled_action_log_probability = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "    \n",
    "    # Return the sampled action and its log probability.\n",
    "    return sampled_action, sampled_action_log_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.3**: Test the `sample_discrete_action` function on a random state using an untrained policy network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "\n",
    "policy_nn = ...\n",
    "\n",
    "state = ...\n",
    "theta = ...                          # Note: theta contains the parameters of the policy network. It is only used here to print its value.\n",
    "action, action_log_probability = ...\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "print(\"state:\", state)\n",
    "print(\"theta:\", theta)\n",
    "print(\"sampled action:\", action)\n",
    "print(\"log probability of the sampled action:\", action_log_probability)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the sample_one_episode function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that in the REINFORCE algorithm, we need to generate a complete trajectory, denoted as $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$. This trajectory includes the states, actions, and rewards at each time step, as outlined in the algorithm at the beginning of Part 1.\n",
    "\n",
    "**Task 1.4**: Your task is to implement the `sample_one_episode` function. This function should play one episode using the given policy $\\pi_\\theta$ and return its rollouts. The function should adhere to a fixed horizon $T$, which represents the maximum number of steps in the episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_one_episode(env: gym.Env,\n",
    "                       policy_nn: PolicyNetwork,\n",
    "                       max_episode_duration: int,\n",
    "                       render: bool = False) -> Tuple[List[NDArray[np.float64]], List[int], List[float], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Execute one episode within the `env` environment utilizing the policy defined by the `policy_nn` parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of the episode.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[List[NDArray[np.float64]], List[int], List[float], List[torch.Tensor]]\n",
    "        The states, actions, rewards, and log probability of action for each time step in the episode.\n",
    "    \"\"\"\n",
    "    state_t, info = env.reset()\n",
    "\n",
    "    episode_states = []\n",
    "    episode_actions = []\n",
    "    episode_log_prob_actions = []\n",
    "    episode_rewards = []\n",
    "    episode_states.append(state_t)\n",
    "\n",
    "    for t in range(max_episode_duration):\n",
    "\n",
    "        if render:\n",
    "            env.render_wrapper.render()\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Sample a discrete action and its log probability from the policy network based on the current state\n",
    "        action_t, log_prob_action_t = ...\n",
    "\n",
    "        # Execute the sampled action in the environment, which returns the new state, reward, and whether the episode has terminated or been truncated\n",
    "        state_t, reward_t, terminated, truncated, info = ...\n",
    "\n",
    "        # Check if the episode is done, either due to termination (reaching a terminal state) or truncation (reaching a maximum number of steps)\n",
    "        done = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        # Append the new state, action, action log probability and reward to their respective lists\n",
    "        episode_states.append(state_t)\n",
    "        episode_actions.append(action_t)\n",
    "        episode_log_prob_actions.append(log_prob_action_t)\n",
    "        episode_rewards.append(reward_t)\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return episode_states, episode_actions, episode_rewards, episode_log_prob_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.5:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "\n",
    "policy_nn = ...\n",
    "episode_states, episode_actions, episode_rewards, episode_log_prob_actions = ...\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "env.close()\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab2_reinforce_untained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement a test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.6**: Implement the `avg_return_on_multiple_episodes` function that test the given policy $\\pi_\\theta$ on `num_episodes` episodes (for fixed horizon $T$) and returns the average reward on the `num_episodes` episodes.\n",
    "\n",
    "The function `avg_return_on_multiple_episodes` is designed to play multiple episodes of a given environment using a specified policy neural network and calculate the average return. It takes as input the environment to play in, the policy neural network to use, the number of episodes to play, the maximum duration of an episode, and an optional parameter to decide whether to render the environment or not. \n",
    "In each episode, it uses the `sample_one_episode` function to play the episode and collect the rewards. The function then returns the average of these cumulated rewards.\n",
    "\n",
    "`avg_return_on_multiple_episodes` will be used for evaluating the performance of a policy over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_return_on_multiple_episodes(env: gym.Env,\n",
    "                                    policy_nn: PolicyNetwork,\n",
    "                                    num_test_episode: int,\n",
    "                                    max_episode_duration: int,\n",
    "                                    render: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Play multiple episodes of the environment and calculate the average return.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to play in.\n",
    "    policy_nn : PolicyNetwork\n",
    "        The policy neural network.\n",
    "    num_test_episode : int\n",
    "        The number of episodes to play.\n",
    "    max_episode_duration : int\n",
    "        The maximum duration of an episode.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default False.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The average return.\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    return_list = []\n",
    "\n",
    "    for _ in range(num_test_episode):\n",
    "        _, _, episode_rewards, _ = ...\n",
    "        return_list.append(sum(episode_rewards))\n",
    "\n",
    "    average_return = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return average_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.7:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "### BEGIN SOLUTION ###\n",
    "\n",
    "policy_nn = ...\n",
    "average_return = ...\n",
    "\n",
    "### END SOLUTION ###\n",
    "\n",
    "print(average_return)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.8**: Implement the `train_reinforce_discrete` function, used to train a policy network using the REINFORCE algorithm in the given environment. This function takes as input the environment, the number of training episodes, the number of tests to perform per episode, the maximum duration of an episode, and the learning rate for the optimizer.\n",
    "\n",
    "The function first initializes a policy network and an Adam optimizer. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return and the log probability of the action taken at that time step to compute the loss, which is the negative of the product of the return and the log probability. This loss is used to update the policy network parameters using gradient ascent.\n",
    "\n",
    "After each training episode, the function tests the current policy by playing a number of test episodes and calculating the average return. This average return is added to a list for monitoring purposes.\n",
    "\n",
    "The function returns the trained policy network and the list of average returns for each episode. This function encapsulates the main loop of the REINFORCE algorithm, including the policy update step. Please refer back to the algorithm outlined at the start of Part 1 for additional context, if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_discrete(env: gym.Env,\n",
    "                             num_train_episodes: int,\n",
    "                             num_test_per_episode: int,\n",
    "                             max_episode_duration: int,\n",
    "                             learning_rate: float) -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode, by default EPISODE_DURATION.\n",
    "    learning_rate : float\n",
    "        The initial step size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average returns for each episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n.item()\n",
    "\n",
    "    policy_nn = PolicyNetwork(state_size, action_size).to(device)\n",
    "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=learning_rate)\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Generate an episode following the current policy\n",
    "        _, _, episode_reward_list, episode_log_prob_action_list = ...\n",
    "\n",
    "        # Iterate over the episode\n",
    "        for t in range(len(episode_reward_list)):\n",
    "            # Calculate the return at time t\n",
    "            future_return = ...\n",
    "\n",
    "            # Convert the future_return to a PyTorch tensor\n",
    "            returns_tensor = ...\n",
    "\n",
    "            # Convert the episode_log_prob_action_list[t] to a PyTorch tensor\n",
    "            log_prob_actions_tensor = ...\n",
    "\n",
    "            # Do gradient ascent\n",
    "            loss = ...\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(env=env,\n",
    "                                                          policy_nn=policy_nn,\n",
    "                                                          num_test_episode=num_test_per_episode,\n",
    "                                                          max_episode_duration=max_episode_duration,\n",
    "                                                          render=False)\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "reinforce_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Train the agent\n",
    "    reinforce_policy_nn, episode_reward_list = train_reinforce_discrete(env=env,\n",
    "                                                                        num_train_episodes=250,\n",
    "                                                                        num_test_per_episode=5,\n",
    "                                                                        max_episode_duration=200,\n",
    "                                                                        learning_rate=0.01)\n",
    "\n",
    "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "reinforce_trains_result_df = pd.DataFrame(np.array(reinforce_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(reinforce_policy_nn, MODELS_DIR / \"lab2_reinforce_policy_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=reinforce_trains_result_df, height=7, aspect=2, alpha=0.5)\n",
    "plt.savefig(PLOTS_DIR / \"lab2_reinforce_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=reinforce_trains_result_df, height=7, aspect=2)\n",
    "plt.savefig(PLOTS_DIR / \"lab2_reinforce_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env, reinforce_policy_nn, 200, render=True)\n",
    "\n",
    "env.close()\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab2_reinforce_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.9**: decrease the learning rate value (e.g. 0.001), increase the number of episodes per training and retrain the agent. What do you observe ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: REINFORCE with Baseline\n",
    "\n",
    "In the basic REINFORCE algorithm, the policy parameters are updated in proportion to the product of the gradient of the policy log-probability and the cumulative reward (return) from a state-action pair. However, this approach can lead to high variance in policy updates, making learning slower and less stable.\n",
    "\n",
    "The baseline is introduced to reduce this variance. It is a value subtracted from the cumulative reward when calculating the policy gradient. The key property of the baseline is that it does not affect the expected value of the gradient estimate, which means it doesn't bias the learning process but reduces the variance of the updates.\n",
    "\n",
    "The baseline can be thought of as a reference point or an \"average\" expectation of reward. By comparing the actual rewards to this baseline, we can determine whether the outcomes of certain actions are better or worse than this \"average\" performance.\n",
    "\n",
    "A common choice for the baseline is the value function of the current policy, $\\hat{V}_{\\boldsymbol{\\omega}}$. By using the value function as a baseline, the algorithm adjusts the policy towards actions that perform better than the average.\n",
    "\n",
    "To incorporate the baseline into REINFORCE, you modify the update rule. Instead of using the total return $G$​ directly, you subtract the baseline value $\\hat{V}_{\\boldsymbol{\\omega}}$ from $G$​ in the policy gradient estimate.\n",
    "\n",
    "By centering the rewards around a baseline, the variance of the policy gradient estimates is reduced. This leads to more stable and efficient learning, as the updates are less noisy and more focused on improving relative to the average performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REINFORCE with Baseline\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    " $\\quad$ A differentiable policy $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    " $\\quad$ A differentiable baseline function $\\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s})$ <br>\n",
    " $\\quad$ A learning rate $\\alpha_1 \\in \\mathbb{R}^+$ for the policy <br>\n",
    " $\\quad$ A learning rate $\\alpha_2 \\in \\mathbb{R}^+$ for the baseline <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    " $\\quad$ Initialize parameters $\\boldsymbol{\\theta} \\in \\mathbb{R}^d$ <br>\n",
    " $\\quad$ Initialize parameters $\\boldsymbol{\\omega} \\in \\mathbb{R}^d$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    " $\\quad$ Generate full trace $\\tau = \\{ \\boldsymbol{s}_0, \\boldsymbol{a}_0, r_1, \\boldsymbol{s}_1, \\boldsymbol{a}_1, \\dots, r_T, \\boldsymbol{s}_T \\}$ following $\\pi_{\\boldsymbol{\\theta}}$ <br>\n",
    " $\\quad$ <b>FOR</b> $~ t=0,\\dots,T-1$ <br>\n",
    "  $\\quad\\quad$ $G \\leftarrow \\sum_{k=t}^{T-1} r_k$ <br>\n",
    "  $\\quad\\quad$ $\\delta_t \\leftarrow G - \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}_t)$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_1 ~ \\delta_t ~ \\nabla_{\\boldsymbol{\\theta}} \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}_t|\\boldsymbol{s}_t)$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\omega} \\leftarrow \\boldsymbol{\\omega} + \\alpha_2 ~ \\delta_t \\nabla_{\\boldsymbol{\\omega}}\\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}_t) $ <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.10**: Implement the `ValueNetwork` ($\\hat{V}_{\\boldsymbol{\\omega}}$ in the algorithm) defined as follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ValueNetwork` is a two-layer fully connected neural network. It takes an input tensor representing the state of the environment and outputs a tensor representing the estimated value of that state. The input tensor's shape should be (N, dim), where N is the number of state vectors in the batch and dim is the dimension of the state vectors.\n",
    "\n",
    "The network has the following components:\n",
    "- `layer1`: This is a linear (fully connected) layer that takes `n_observations` as input and outputs `nn_l1` neurons.\n",
    "- `layer2`: This is another linear layer that takes `nn_l1` neurons as input and outputs a single value.\n",
    "- `forward` method: This method defines the forward pass of the network. It takes a state tensor as input and returns a tensor representing the estimated value of the state. It first applies the ReLU activation function to the output of the first layer, and then applies the second linear layer to get the final output.\n",
    "\n",
    "This network is quite simple and may not perform well on complex tasks with large state spaces. However, it can be a good starting point for simple reinforcement learning tasks, and can be easily extended with more layers or different types of layers (such as convolutional layers for image inputs) to handle more complex tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A two-layer fully connected network that estimates the value of a state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_observations : int\n",
    "        The number of observations in the state.\n",
    "    nn_l1 : int, optional\n",
    "        The number of neurons in the first layer, by default 16\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        The first fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        The second fully connected layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, nn_l1: int = 16):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        self.layer1 = ...\n",
    "        self.layer2 = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "\n",
    "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_tensor : torch.Tensor\n",
    "            The input tensor representing the state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor representing the value of the state.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        x = ...\n",
    "        x = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the train function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1.11**: Implement the `train_reinforce_baseline_discrete` function, used to train a policy network and a value network using the REINFORCE with baseline algorithm in a given environment.\n",
    "\n",
    "The function first initializes a policy network and a value network, along with their respective Adam optimizers. Then, for each training episode, it generates an episode using the current policy and calculates the return at each time step. It uses this return, the log probability of the action taken at that time step, and the estimated value of the state to compute the policy and value losses. These losses are used to update the policy and value network parameters using gradient ascent. The value loss is typically defined as the squared difference between the estimated return and the actual return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_reinforce_baseline_discrete(env: gym.Env,\n",
    "                                      num_train_episodes: int,\n",
    "                                      num_test_per_episode: int,\n",
    "                                      max_episode_duration: int,\n",
    "                                      policy_learning_rate: float,\n",
    "                                      value_learning_rate: float) -> Tuple[PolicyNetwork, List[float]]:\n",
    "    \"\"\"\n",
    "    Train a policy using the REINFORCE with baseline algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train in.\n",
    "    num_train_episodes : int\n",
    "        The number of training episodes.\n",
    "    num_test_per_episode : int\n",
    "        The number of tests to perform per episode.\n",
    "    max_episode_duration : int\n",
    "        The maximum length of an episode.\n",
    "    policy_learning_rate : float\n",
    "        The policy learning rate.\n",
    "    value_learning_rate : float\n",
    "        The value learning rate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[PolicyNetwork, List[float]]\n",
    "        The final trained policy and the average returns for each episode.\n",
    "    \"\"\"\n",
    "    episode_avg_return_list = []\n",
    "\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n.item()\n",
    "\n",
    "    policy_nn = PolicyNetwork(state_size, action_size).to(device)\n",
    "    policy_optimizer = torch.optim.Adam(policy_nn.parameters(), lr=policy_learning_rate)\n",
    "\n",
    "    value_nn = ValueNetwork(state_size).to(device)\n",
    "    value_optimizer = torch.optim.Adam(value_nn.parameters(), lr=value_learning_rate)\n",
    "\n",
    "    for episode_index in tqdm(range(num_train_episodes)):\n",
    "\n",
    "        # Generate an episode following the current policy\n",
    "        episode_state_list, _, episode_reward_list, episode_log_prob_action_list = sample_one_episode(env=env,\n",
    "                                                                                                      policy_nn=policy_nn,\n",
    "                                                                                                      max_episode_duration=max_episode_duration)\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        # Test the current policy\n",
    "        test_avg_return = avg_return_on_multiple_episodes(env=env,\n",
    "                                                          policy_nn=policy_nn,\n",
    "                                                          num_test_episode=num_test_per_episode,\n",
    "                                                          max_episode_duration=max_episode_duration,\n",
    "                                                          render=False)\n",
    "\n",
    "        # Monitoring\n",
    "        episode_avg_return_list.append(test_avg_return)\n",
    "\n",
    "    return policy_nn, episode_avg_return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "reinforce_baseline_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Train the agent\n",
    "    reinforce_baseline_policy_nn, episode_reward_list = train_reinforce_baseline_discrete(env=env,\n",
    "                                                                                          num_train_episodes=250,\n",
    "                                                                                          num_test_per_episode=5,\n",
    "                                                                                          max_episode_duration=200,\n",
    "                                                                                          policy_learning_rate=0.02,\n",
    "                                                                                          value_learning_rate=0.02)\n",
    "\n",
    "    reinforce_baseline_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    reinforce_baseline_trains_result_list[1].extend(episode_reward_list)\n",
    "    reinforce_baseline_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "reinforce_baseline_trains_result_df = pd.DataFrame(np.array(reinforce_baseline_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "reinforce_baseline_trains_result_df[\"agent\"] = \"REINFORCE baseline\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(reinforce_baseline_policy_nn, MODELS_DIR / \"lab2_reinforce_baseline_policy_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=reinforce_baseline_trains_result_df, height=7, aspect=2, alpha=0.5)\n",
    "plt.savefig(PLOTS_DIR / \"lab2_reinforce_cartpole_trains_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=reinforce_baseline_trains_result_df, height=7, aspect=2)\n",
    "plt.savefig(PLOTS_DIR / \"lab2_reinforce_cartpole_trains_result_agg.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test final policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n.item()\n",
    "\n",
    "episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env, reinforce_policy_nn, 200, render=True)\n",
    "\n",
    "env.close()\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab2_reinforce_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus section 1: Implementing REINFORCE for Continuous Action Spaces (Lunar Lander)\n",
    "\n",
    "The REINFORCE agent we've implemented so far is designed for environments with discrete action spaces. However, policy-based methods can effectively handle large, even continuous, action spaces. Instead of calculating learned probabilities for each possible action, we learn the statistics of the probability distribution. For instance, if the action set comprises real numbers, actions could be chosen from a normal (Gaussian) distribution.\n",
    "\n",
    "To create a policy parameterization, we can define the policy as the normal probability density over a real-valued scalar action. The mean and standard deviation of this distribution are determined by parametric function approximators (the `PolicyNetwork` neural network) that depend on the state.\n",
    "\n",
    "We can divide the policy’s parameter vector, $\\boldsymbol{\\theta} = [ \\boldsymbol{\\theta}_\\mu, \\boldsymbol{\\theta}_\\sigma ]^\\top$, into two parts: one for approximating the mean and the other for approximating the standard deviation.\n",
    "\n",
    "**Task 1.12**: Modify the `PolicyNetwork`, `sample_discrete_action`, and `train_reinforce_discrete` functions to make REINFORCE compatible with the *LunarLander-v2* environment. Remember to set `continuous=True` in the `gym.make` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actor Critic with bootstrapping\n",
    "\n",
    "<b>REQUIRE</b> <br>\n",
    " $\\quad$ A policy $\\pi_{\\boldsymbol{\\theta}}$ and a value function $V_{\\boldsymbol{\\omega}}$ <br>\n",
    " $\\quad$ A learning rate $\\alpha_1$ for the critic and $\\alpha_2$ for the actor <br>\n",
    "<b>INITIALIZATION</b> <br>\n",
    " $\\quad$ $\\boldsymbol{\\theta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$ <br>\n",
    " $\\quad$ $\\boldsymbol{\\omega} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_n)$ <br>\n",
    "<br>\n",
    "<b>FOR EACH</b> episode <br>\n",
    " $\\quad$ $\\boldsymbol{s} \\leftarrow \\text{env.reset}()$ <br>\n",
    " $\\quad$ <b>DO</b> <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{a} \\sim \\pi_{\\boldsymbol{\\theta}}(\\cdot | \\boldsymbol{s})$ <br>\n",
    "  $\\quad\\quad$ $r, \\boldsymbol{s'} \\leftarrow \\text{env.step}(\\boldsymbol{a})$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\omega} \\leftarrow \\boldsymbol{\\omega} + \\alpha_1 \\left[ r + \\gamma \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s'}) - \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}) \\right] \\nabla_{\\boldsymbol{\\omega}} \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s})$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{\\theta} \\leftarrow \\boldsymbol{\\theta} + \\alpha_2 \\left[ \\nabla_{\\boldsymbol{\\theta}} ~ \\ln \\pi_{\\boldsymbol{\\theta}}(\\boldsymbol{a}|\\boldsymbol{s}) \\times \\hat{V}_{\\boldsymbol{\\omega}}(\\boldsymbol{s}) \\right]$ <br>\n",
    "  $\\quad\\quad$ $\\boldsymbol{s} \\leftarrow \\boldsymbol{s'}$ <br>\n",
    " $\\quad$ <b>UNTIL</b> $\\boldsymbol{s}$ is final <br>\n",
    "<br>\n",
    "<b>RETURN</b> $\\boldsymbol{\\theta}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus section 2: Implementing the simple Actor Critic algorithm to solve the Cartpole problem\n",
    "\n",
    "**Task**: Implement the Actor Critic algorithm presented above to solve the Cartpole problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION ###\n",
    "\n",
    "...\n",
    "\n",
    "### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section 3: Monitoring the training process with Weights & Biases on top of Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab, we have seen how to monitor the training process using Tensorboard and Weights & Biases separately. In this bonus section, we will see how to use Weights & Biases on top of Tensorboard to monitor the training process. Then, with only one line of code, you can switch between the two tools without changing your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    lr: float\n",
    "    batch_size: int\n",
    "\n",
    "hparams = Hyperparameters(lr=0.01, batch_size=32)\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "\n",
    "    # wandb.login()\n",
    "    wandb.init(\n",
    "        project=\"INF639\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "except:\n",
    "    warnings.warn(\"Wandb is not installed\")\n",
    "\n",
    "\n",
    "# Simple example: model and data\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Creating a model, optimizer, and loss function\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=hparams.lr)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initializing TensorBoard SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training example\n",
    "for epoch in range(10):\n",
    "    # Fake data\n",
    "    inputs = torch.randn(hparams.batch_size, 10)\n",
    "    targets = torch.randn(hparams.batch_size, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log the loss values to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "\n",
    "# Closing SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section 4: Stable Baselines 3 and MuJoCo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to be able to write reinforcement learning (RL) algorithms from scratch, as it helps in deeply understanding the underlying concepts, mechanisms, and trade-offs involved in RL. This skill is essential for debugging, optimizing, and innovating beyond standard approaches. However, in many practical cases, leveraging well-established libraries like Stable Baselines 3 (SB3) can be incredibly useful for solving real-world RL problems. SB3 provides tested, efficient, and optimized implementations of various algorithms, saving time and ensuring reliability when applying RL to concrete tasks. Knowing how to balance custom implementations with using proven tools like SB3 is key to efficiently developing and deploying RL solutions.\n",
    "\n",
    "In this bonus section, we will first use SB3 to solve the Cartpole environment. We will then move on to the MuJoCo environment, which is more complex and requires a more advanced algorithm to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no exercise in this section. You can run the code and observe the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PPO agent on Cartpole with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a very concise example. The code below trains a PPO agent on the Cartpole environment using Stable Baselines 3. The code is very simple and requires only a few lines to set up the environment, create the agent, and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_vec_env(\"CartPole-v1\", n_envs=4)   # Parallel environments\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=100_000)\n",
    "model.save(MODELS_DIR / \"lab2_ppo_cartpole\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then evaluate the agent by running it in the environment and generate a video of the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_cartpole_sb3_ppo_basic\"\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = PPO.load(MODELS_DIR / \"lab2_ppo_cartpole\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(200):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PPO agent on Cartpole with Stable Baselines 3 and evaluation callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now add an evaluation callback to the training process. This callback will evaluate the agent at regular intervals during training and save the evaluation results. This allows us to monitor the agent's performance during training and analyze how it improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_vec_env(\"CartPole-v1\", n_envs=4)   # Parallel environments\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Define a callback to periodically evaluate the model\n",
    "eval_env = gym.make(\"CartPole-v1\")\n",
    "eval_callback = EvalCallback(\n",
    "    eval_env,\n",
    "    best_model_save_path=MODELS_DIR / \"lab2_ppo_cartpole_best\",\n",
    "    log_path=LOGS_DIR / \"lab2_ppo_cartpole\",\n",
    "    eval_freq=5000,\n",
    "    deterministic=True,\n",
    "    render=False\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=50_000,\n",
    "    callback=eval_callback\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the evaluation results (results_array will contain the following keys: timesteps, results, ep_lengths)\n",
    "results_array = np.load(LOGS_DIR / \"lab2_ppo_cartpole\" / \"evaluations.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the evaluation results\n",
    "plt.plot(results_array['timesteps'], results_array['results'])\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean reward\")\n",
    "plt.title(\"Evaluation results\")\n",
    "plt.savefig(PLOTS_DIR / \"lab2_ppo_cartpole_eval.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the evaluation results\n",
    "plt.plot(results_array['timesteps'], results_array['ep_lengths'])\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Episode Length\")\n",
    "plt.title(\"Episode Length vs Timesteps\")\n",
    "plt.savefig(PLOTS_DIR / \"lab2_ppo_cartpole_ep_lenghts.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PPO agent on Cartpole with Stable Baselines 3 monitoring the training process with Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now monitor the training process using Tensorboard. This allows us to visualize various metrics such as the reward, the loss, and the entropy of the policy during training. This can help us understand how the agent learns and how the training process evolves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = make_vec_env(\"CartPole-v1\", n_envs=4)   # Parallel environments\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_ppo_cartpole_tb\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=50_000,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PPO agent on Cartpole with Stable Baselines 3 monitoring the training process with Weights & Biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now monitor the training process using Weights & Biases. Videos of the agent's performance will be saved and can be viewed in the Weights & Biases dashboard. This allows us to visualize the agent's behavior during training and analyze how it improves over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_ppo_cartpole\"\n",
    "\n",
    "# env = make_vec_env(\"CartPole-v1\", n_envs=4)   # Parallel environments\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_ppo_cartpole_tb\"\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "model.save(MODELS_DIR / \"lab2_ppo_cartpole\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose different policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have used the default policy for the PPO agent. However, Stable Baselines 3 provides several other policies that can be used with the PPO agent. In this section, we will experiment with different policies and observe how they affect the agent's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "    actor_num_hidden_layer: int\n",
    "    critic_num_hidden_layer: int\n",
    "    actor_hidden_layer_units: int\n",
    "    critic_hidden_layer_units: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=50_000,\n",
    "    actor_num_hidden_layer=2,\n",
    "    critic_num_hidden_layer=2,\n",
    "    actor_hidden_layer_units=32,\n",
    "    critic_hidden_layer_units=32\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_ppo_cartpole_custom_policy\"\n",
    "\n",
    "# env = make_vec_env(\"CartPole-v1\", n_envs=4)   # Parallel environments\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "net_arch_dict = {\n",
    "    \"pi\": [hparams.actor_hidden_layer_units for _ in range(hparams.actor_num_hidden_layer)],\n",
    "    \"vf\": [hparams.critic_hidden_layer_units for _ in range(hparams.critic_num_hidden_layer)],\n",
    "}\n",
    "\n",
    "# Define a custom architecture for the networks\n",
    "policy_kwargs = dict(\n",
    "    # net_arch=[dict(pi=[128, 128], vf=[128, 128])],  # Network architecture with 2 layers of 128 units for the actor and the critic\n",
    "    #net_arch=[dict(pi=[hparams.actor_hidden_layer_units], vf=[hparams.critic_hidden_layer_units])],                # Network architecture with 1 layer of 16 units for the actor and the critic\n",
    "    net_arch=net_arch_dict,                # Network architecture with 1 layer of 16 units for the actor and the critic\n",
    "    activation_fn=torch.nn.Tanh                       # Activation function for each layer (here Tanh)\n",
    ")\n",
    "\n",
    "# Initialize the PPO model with a custom architecture\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    policy_kwargs=policy_kwargs,     # Use the custom policy\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_ppo_cartpole_tb\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps,\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(MODELS_DIR / \"lab2_ppo_cartpole_custom\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hands on MuJoCo environment\n",
    "\n",
    "We now switch to the MuJoCo environment, which is more complex and requires a more advanced algorithm to solve.\n",
    "\n",
    "MuJoCo environments are widely used benchmarks for testing reinforcement learning algorithms. They consist of 11 robotics tasks, all of which involve continuous control with continuous action spaces.\n",
    "\n",
    "You can find a detailed description of the MuJoCo environments [here](https://gymnasium.farama.org/environments/mujoco/).\n",
    "\n",
    "In this lab, we will work with the `Ant-v4` environment. In this task, the agent controls a four-legged robot, guiding it to move forward as quickly as possible. The agent earns a reward based on the distance the robot covers.\n",
    "\n",
    "This environment is described [here](https://gymnasium.farama.org/environments/mujoco/ant/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Ant-v4', render_mode=\"rgb_array\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space\n",
    "\n",
    "print(f\"State space : { env.observation_space }\")\n",
    "print(f\"Action space : { env.action_space }\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells and check different basic \n",
    "policies (for instance randomly drawn actions) to discover the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_mujoco_zero_policy\"\n",
    "\n",
    "env = gym.make('Ant-v4', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for _ in range(100):\n",
    "    if not done:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = np.zeros(env.action_space.shape)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_mujoco_one_policy\"\n",
    "\n",
    "env = gym.make('Ant-v4', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for _ in range(100):\n",
    "    if not done:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = np.ones(env.action_space.shape)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_mujoco_minus_one_policy\"\n",
    "\n",
    "env = gym.make('Ant-v4', render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for _ in range(100):\n",
    "    if not done:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = np.full(env.action_space.shape, -1.)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_mujoco_random_uniform_policy\"\n",
    "\n",
    "env = gym.make('Ant-v4', render_mode='rgb_array', terminate_when_unhealthy=False)\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for _ in range(100):\n",
    "    if not done:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_DIRNAME = \"lab2_mujoco_random_normal_policy\"\n",
    "\n",
    "env = gym.make('Ant-v4', render_mode='rgb_array', terminate_when_unhealthy=False)\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for _ in range(100):\n",
    "    if not done:\n",
    "        print(\".\", end=\"\")\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = np.random.normal(size=env.action_space.shape)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Read the [MuJoCo Ant environment documentation](https://gymnasium.farama.org/environments/mujoco/ant/) to understand the state and action spaces, as well as the reward structure and the termination conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a PPO agent on MuJoCo with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639_lab2_mujoco_ant\",\n",
    "        name=\"ppo\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_mujoco_sb3_ppo_policy\"\n",
    "\n",
    "# Create the Ant-v4 environment from Gymnasium\n",
    "env_id = 'Ant-v4'\n",
    "# env = make_vec_env(env_id, n_envs=4)  # Use multiple environments for parallelism\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(         # Record the video of the environment (visible on Weight & Biases)\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_ppo_mujoco_tb\"\n",
    ")\n",
    "\n",
    "# # Define a callback to periodically evaluate the model\n",
    "# eval_env = gym.make(env_id)\n",
    "# eval_callback = EvalCallback(\n",
    "#     eval_env,\n",
    "#     best_model_save_path=MODELS_DIR / \"lab2_ppo_mujoco_best\",\n",
    "#     log_path=LOGS_DIR / \"lab2_ppo_mujoco\",\n",
    "#     eval_freq=5000,\n",
    "#     deterministic=True,\n",
    "#     render=False\n",
    "# )\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    # progress_bar=True,\n",
    "    # callback=eval_callback,\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_ppo_mujoco\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = PPO.load(MODELS_DIR / \"lab2_ppo_mujoco\")\n",
    "# model = PPO.load(MODELS_DIR / \"lab2_ppo_mujoco_best\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a A2C agent on MuJoCo with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639_lab2_mujoco_ant\",\n",
    "        name=\"a2c\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_mujoco_sb3_a2c_policy\"\n",
    "\n",
    "# Create the Ant-v4 environment from Gymnasium\n",
    "env_id = 'Ant-v4'\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(         # Record the video of the environment (visible on Weight & Biases)\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "# Initialize the A2C model\n",
    "model = A2C(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_a2c_mujoco_tb\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_a2c_mujoco\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = A2C.load(MODELS_DIR / \"lab2_a2c_mujoco\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a DDPG agent on MuJoCo with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DDPG\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639_lab2_mujoco_ant\",\n",
    "        name=\"ddpg\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_mujoco_sb3_ddpg_policy\"\n",
    "\n",
    "# Create the Ant-v4 environment from Gymnasium\n",
    "env_id = 'Ant-v4'\n",
    "# env = make_vec_env(env_id, n_envs=4)  # Use multiple environments for parallelism\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(         # Record the video of the environment (visible on Weight & Biases)\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "# Initialize the DDPG model\n",
    "model = DDPG(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_ddpg_mujoco_tb\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_ddpg_mujoco\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = DDPG.load(MODELS_DIR / \"lab2_ddpg_mujoco\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a TD3 agent on MuJoCo with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import TD3\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639_lab2_mujoco_ant\",\n",
    "        name=\"td3\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_mujoco_sb3_td3_policy\"\n",
    "\n",
    "# Create the Ant-v4 environment from Gymnasium\n",
    "env_id = 'Ant-v4'\n",
    "# env = make_vec_env(env_id, n_envs=4)  # Use multiple environments for parallelism\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(         # Record the video of the environment (visible on Weight & Biases)\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "# Initialize the TD3 model\n",
    "model = TD3(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_td3_mujoco_tb\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_td3_mujoco\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = TD3.load(MODELS_DIR / \"lab2_td3_mujoco\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a SAC agent on MuJoCo with Stable Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "\n",
    "@dataclass\n",
    "class Hyperparameters:\n",
    "    total_timesteps: int\n",
    "\n",
    "hparams = Hyperparameters(\n",
    "    total_timesteps=1_000_000\n",
    ")\n",
    "\n",
    "# If wandb is installed, initialize it\n",
    "if \"wandb\" in sys.modules:\n",
    "    wandb.init(\n",
    "        project=\"INF639_lab2_mujoco_ant\",\n",
    "        name=\"sac\",\n",
    "        sync_tensorboard=True,\n",
    "        monitor_gym=True,\n",
    "        config=vars(hparams)\n",
    "    )\n",
    "\n",
    "VIDEO_DIRNAME = \"lab2_mujoco_sb3_sac_policy\"\n",
    "\n",
    "# Create the Ant-v4 environment from Gymnasium\n",
    "env_id = 'Ant-v4'\n",
    "# env = make_vec_env(env_id, n_envs=4)  # Use multiple environments for parallelism\n",
    "env = gym.make(env_id, render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(         # Record the video of the environment (visible on Weight & Biases)\n",
    "    env,\n",
    "    FIGS_DIR / VIDEO_DIRNAME,\n",
    "    disable_logger=True\n",
    ")\n",
    "\n",
    "# Initialize the SAC model\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=0,\n",
    "    tensorboard_log=LOGS_DIR / \"lab2_sac_mujoco_tb\"\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=hparams.total_timesteps\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save(MODELS_DIR / \"lab2_sac_mujoco\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode='rgb_array')\n",
    "env = gym.wrappers.RecordVideo(env, FIGS_DIR / VIDEO_DIRNAME)\n",
    "\n",
    "model = SAC.load(MODELS_DIR / \"lab2_sac_mujoco\")\n",
    "\n",
    "observation, info = env.reset()\n",
    "for _ in range(500):\n",
    "    action, _states = model.predict(observation)\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "Video(FIGS_DIR / VIDEO_DIRNAME / \"rl-video-episode-0.mp4\", embed=True, html_attributes=\"controls autoplay loop\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Value-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2024](https://moodle.polytechnique.fr/course/view.php?id=19358) Lab session #1\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of the most renowned value-based reinforcement learning techniques, specifically *Deep Q-Networks* and its enhancements.\n",
    "\n",
    "In this Python notebook, you will implement and evaluate *Deep Q-Networks* (DQN) and its various adaptations.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "\n",
    "This tutorial has been tested with Python 3.10 and Python 3.11.\n",
    "\n",
    "It is important to note that **Bonus Section 4: *Test and train a DQN agent to play Atari games*** [is not compatible with Python 3.12](https://github.com/Farama-Foundation/Gymnasium/issues/1081) (the latest stable version of Python). If you plan to complete this bonus section locally on your machine rather than on Google Colab, make sure you have Python 3.10 or 3.11 installed.\n",
    "\n",
    "If you are using Python 3.12 and prefer not to run this notebook on Google Colab, an alternative solution via Docker will be provided at the beginning of Bonus Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following survey to help us improve this lab session: https://moodle.polytechnique.fr/mod/questionnaire/view.php?id=535310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Deep value-based reinforcement learning\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large.\n",
    "\n",
    "While DQN was designed to tackle large environments like Atari games, the primary focus of this lab is to delve into the underlying algorithms, understand them thoroughly, and evaluate them comprehensively. It's important to note that working with not-so-deep networks captures the essence of deep reinforcement learning, excluding the computational expense. The transition from tabular Q-learning to DQN involves significant implications, primarily due to the ability of DQN to handle high-dimensional state spaces. Moving from DQN to very-deep-DQN is primarily a matter of scale and computational resources. The core principles remain the same, and understanding these principles is the key to mastering reinforcement learning, regardless of the complexity of the network used.\n",
    "For these reasons, in this lab, we will focus on studying the CartPole environment. The CartPole problem is a classic in reinforcement learning, and it provides a simpler and more manageable context for understanding the principles of DQN. The convergence in the CartPole environment is much faster than in Atari games - typically within a minute, as opposed to approximately hours on a well-equipped personal computer for Atari games. This allows us to experiment and iterate more quickly, facilitating a deeper understanding of the algorithms at play.\n",
    "\n",
    "We will therefore focus on algorithms in a simple environment. However, once the key concepts are mastered and tested in these settings, Bonus Section 4 will offer you the opportunity to refine your technical skills by applying them to the game Breakout in the Atari environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `torch`, `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the provided [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) and [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) files.\n",
    "\n",
    "- [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains the minimal dependencies required to run this notebook without the optional sections (e.g. the Atari environment, Aim, Tensorboard, Optuna, Weights&Bias, ...).\n",
    "- [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains all the dependencies required to run this notebook with all the optional sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJfFvm-4mOI2"
   },
   "outputs": [],
   "source": [
    "#! apt install xvfb x11-utils && pip install gymnasium pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, first download the [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) or [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) depending on whether you want to run the optional sections of this notebook or not (c.f. *Setup the Python environment* section above).\n",
    "\n",
    "Ensure it is located in the same directory as this notebook. Next, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version.\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "env\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work jdhp/inf639-lab1:latest\n",
    "```\n",
    "\n",
    "If you encounter an error during the notebook's execution indicating that writing a file is not possible, this issue may stem from the user ID within the container lacking the necessary permissions in the project directory. This problem can be resolved by modifying the directory's permissions, for example, using the command:\n",
    "\n",
    "```bash\n",
    "chmod 777 . figs models\n",
    "rm -rf figs/*.gif\n",
    "rm -rf figs/*.png\n",
    "rm -rf models/*.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "# from inf639 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "MODELS_DIR = Path(\"models/\")   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir()\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to visualize the episodes as animated GIFs. Run the cell below to enable this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAFnXFZcrr-9"
   },
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            gif_path = filename.with_suffix('.gif')\n",
    "            imageio.mimsave(gif_path, [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(gif_path,'rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed. \n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined (the `NUMBER_OF_TRAININGS` variable is commented out at the beginning of these cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU.\n",
    "\n",
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful for CartPole (but useful for Atari Breakout in *Bonus section 4*) because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a recent GPU (e.g. RTX 4060 Ti 16G) and want to use it, you may need to install a specific version of PyTorch compatible with your Cuda version (e.g. Cuda 12.4). For this, you will have to edit the `requirements.txt` file and replace the current version of PyTorch with the one compatible with your Cuda version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section 1: Monitoring the training process with Tensorboard and Aim\n",
    "\n",
    "Monitoring the training process is crucial for understanding the agent's learning progress. In this section, we will introduce two open-source tools that can help you visualize the training process: Tensorboard and Aim.\n",
    "\n",
    "A third tool, [Weights & Biases (W&B)](https://wandb.ai/site) will be presented in *Bonus section 3*.\n",
    "\n",
    "Tensorflow and Aim are particularly useful for monitoring the training process on a local machine, as they do not require a server to run.\n",
    "On the opposite, W&B is a cloud-based solution that allows you to monitor the training process from anywhere.\n",
    "Other solutions will be covered in the next tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with Tensorboard\n",
    "\n",
    "TensorBoard is a visualization tool developed by Google, primarily used to track and visualize metrics during the training of deep learning models. While initially designed for TensorFlow, it can also be integrated with **PyTorch** through the **torch.utils.tensorboard** API. This allows real-time tracking of metrics such as losses, accuracy, computation graphs, and even examining images, weight distributions, histograms, and more.\n",
    "\n",
    "The following cell demonstrates how to use TensorBoard with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Simple example: model and data\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Creating a model, optimizer, and loss function\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initializing TensorBoard SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training example\n",
    "for epoch in range(100):\n",
    "    # Fake data\n",
    "    inputs = torch.randn(32, 10)\n",
    "    targets = torch.randn(32, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the loss values to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "\n",
    "# Closing SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SummaryWriter`** is the central tool for logging data to TensorBoard. It captures metric values, like loss, for each iteration/epoch.\n",
    "\n",
    "This example trains a simple model on fake data (32 samples with 10 features) over 100 epochs, and at each epoch, the loss value is sent to TensorBoard via `writer.add_scalar`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:6006/).\n",
    "\n",
    "If you are using Google Colab, you can use the **TensorBoard magic** to visualize the data directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mor information, check the Official Documentation:\n",
    "- [TensorBoard for PyTorch Documentation](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "- [Official TensorBoard Documentation (General)](https://www.tensorflow.org/tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with AIM\n",
    "\n",
    "Aim is an open-source alternative to TensorBoard for managing and visualizing machine learning experiments.\n",
    "Designed to be simpler and more flexible, **Aim** allows tracking and analyzing metrics, hyperparameters, model outputs, and more for projects using PyTorch, TensorFlow, or other frameworks.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" width=\"600px\"></img>\n",
    "\n",
    "The following cell demonstrates how to use Aim with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import aim\n",
    "\n",
    "# Initialize Aim\n",
    "run = aim.Run()\n",
    "\n",
    "# set training hyperparameters\n",
    "run['hparams'] = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Simple model example\n",
    "model = torch.nn.Linear(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    inputs = torch.randn(10, 1)\n",
    "    targets = torch.randn(10, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the 'loss' metric\n",
    "    run.track(loss.item(), name='loss', epoch=epoch)    # Log metric with Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example trains a simple model on fake data over 10 epochs, and at each epoch, the loss value is sent to Aim via `run.track`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "aim up\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:43800/).\n",
    "\n",
    "Or alternatively, you can use the **Aim magic** to [visualize the data directly in the notebook](https://aimstack.readthedocs.io/en/latest/using/jupyter_notebook_ui.html) (for instance if you are using Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aim\n",
    "%aim up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, check the official Documentation:\n",
    "- [Aim Documentation](https://aimstack.readthedocs.io/en/latest/)\n",
    "- [GitHub Aim](https://github.com/aimhubio/aim)\n",
    "- [Quick start](https://github.com/aimhubio/aim#-quick-start)\n",
    "- [Getting started](https://aimstack.readthedocs.io/en/latest/quick_start/setup.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Hands on Cart Pole environment [reminder]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of focusing on the algorithms, we will use standard environments provided by the Gymnasium suite.\n",
    "Gymnasium provides controllable environments (https://gymnasium.farama.org/environments/classic_control/) for research in reinforcement learning.\n",
    "Especially, we will try to solve the CartPole-v1 environment (c.f. https://gymnasium.farama.org/environments/classic_control/cart_pole/) which offers a continuous state space and discrete action space.\n",
    "The Cart Pole task consists in maintaining a pole in a vertical position by moving a cart on which the pole is attached with a joint.\n",
    "No friction is considered.\n",
    "The task is supposed to be solved if the pole stays up-right (within 15 degrees) for 200 steps in average over 100 episodes while keeping the cart position within reasonable bounds.\n",
    "The state is given by $\\{x,\\frac{\\partial x}{\\partial t},\\omega,\\frac{\\partial \\omega}{\\partial t}\\}$ where $x$ is the position of the cart and $\\omega$ is the angle between the pole and vertical position.\n",
    "There are only two possible actions: $a \\in \\{0, 1\\}$ where $a = 0$ means \"push the cart to the LEFT\" and $a = 1$ means \"push the cart to the RIGHT\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Hands on Cart Pole\n",
    "\n",
    "**Task 1:** refer to the following link [CartPole Environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/) to familiarize yourself with the CartPole environment if you are not already.\n",
    "\n",
    "**Note:** for a refresher on the key concepts of Gymnasium, you can visit this [Basic Usage Guide](https://gymnasium.farama.org/content/basic_usage/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { env.observation_space }\")\n",
    "print(f\"Action space size is: { env.action_space }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2:** Run the following cells and check different basic \n",
    "policies (for instance constant actions or randomly drawn actions) to discover the CartPole environment.\n",
    "Although this environment has easy dynamics that can be computed analytically, we will solve this problem with Policy Gradient based Reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test the CartPole environment with a constant policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = 0\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "observation, info = env.reset()\n",
    "done = False\n",
    "\n",
    "for t in range(50):\n",
    "    env.render_wrapper.render()\n",
    "\n",
    "    if not done:\n",
    "        print(observation)\n",
    "    else:\n",
    "        print(\"x\", end=\"\")\n",
    "\n",
    "    action = 1\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "print()\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the CartPole environment with a random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex1random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Deep value-based reinforcement learning with Deep Q-Networks (DQN) [reminder]\n",
    "\n",
    "In this part, we will begin our exploration of deep reinforcement learning with Deep Q-Networks (DQN), a famous value-based method.\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement a naive value-based deep reinforcement learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to write a naive implementation of a version of Q-Learning, where the Q-function is approximated by a neural network. This approach combines traditional Q-Learning with the power of function approximation provided by neural networks, allowing us to handle environments with large state spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}}$ $)$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{\\omega} \\leftarrow \\mathbf{\\omega} + \\alpha \\left[ r + \\gamma \\max_{\\mathbf{a}^\\star \\in \\mathcal{A}}\\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'})_{\\mathbf{a}^\\star} - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}} \\right] ~ \\nabla_{\\mathbf{\\omega}} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s})_{\\mathbf{a}}$ <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Q-network\n",
    "\n",
    "The Q-Network is used to approximate the action value function, which gives the expected future reward for taking a particular action in a particular state. The network is trained to minimize the difference between its predicted Q-values and the actual return received.\n",
    "\n",
    "Here, the Q-network is a simple feedforward neural network with two hidden layers. The input to the network is the observations, and the output is the Q-value for each action.\n",
    "Each hidden layer uses the ReLU activation function.\n",
    "The output layer uses the linear activation function.\n",
    "\n",
    "**Task 2.1:** implement the constructor and the `forward` method of the Q-network we will use in our RL agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer.\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer.\n",
    "    layer3 : torch.nn.Linear\n",
    "        Third fully connected layer.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the QNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons on the first layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons on the second layer.\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(n_observations, nn_l1)\n",
    "        self.layer2 = torch.nn.Linear(nn_l1, nn_l2)\n",
    "        self.layer3 = torch.nn.Linear(nn_l2, n_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the QNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # [...]\n",
    "        x = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement an inference function\n",
    "\n",
    "**Task 2.2:** Your next assignment is to complete the function below, which will be used to evaluate the performance of an agent in a simulated environment over one or multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_q_network_agent(env: gym.Env, q_network: torch.nn.Module, num_episode: int = 1, render: bool = True) -> List[int]:\n",
    "    \"\"\"\n",
    "    Test a naive agent in the given environment using the provided Q-network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment in which to test the agent.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to use for decision making.\n",
    "    num_episode : int, optional\n",
    "        The number of episodes to run, by default 1.\n",
    "    render : bool, optional\n",
    "        Whether to render the environment, by default True.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[int]\n",
    "        A list of rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_id in range(num_episode):\n",
    "\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render_wrapper.render()\n",
    "\n",
    "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "            ### BEGIN SOLUTION ###\n",
    "\n",
    "            q_values = ...\n",
    "            action = ...\n",
    "\n",
    "            ### END SOLUTION ###\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2.3:** Test this function on the untrained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab6_dqn_naive_untained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the epsilon greedy function\n",
    "\n",
    "**Task 2.4:** Now, let's proceed to implement the epsilon-greedy strategy, which is a crucial component in balancing exploration and exploitation during the learning process of our reinforcement learning agent. To accomplish this, complete the `__call__` function in the following code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An Epsilon-Greedy policy.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    epsilon : float\n",
    "        The initial probability of choosing a random action.\n",
    "    epsilon_min : float\n",
    "        The minimum probability of choosing a random action.\n",
    "    epsilon_decay : float\n",
    "        The decay rate for the epsilon value after each action.\n",
    "    env : gym.Env\n",
    "        The environment in which the agent is acting.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-Network used to estimate action values.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    __call__(state: np.ndarray) -> np.int64\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "    decay_epsilon()\n",
    "        Decay the epsilon value after each action.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 epsilon_start: float,\n",
    "                 epsilon_min: float,\n",
    "                 epsilon_decay:float,\n",
    "                 env: gym.Env,\n",
    "                 q_network: torch.nn.Module):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of EpsilonGreedy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        epsilon_start : float\n",
    "            The initial probability of choosing a random action.\n",
    "        epsilon_min : float\n",
    "            The minimum probability of choosing a random action.\n",
    "        epsilon_decay : float\n",
    "            The decay rate for the epsilon value after each episode.\n",
    "        env : gym.Env\n",
    "            The environment in which the agent is acting.\n",
    "        q_network : torch.nn.Module\n",
    "            The Q-Network used to estimate action values.\n",
    "        \"\"\"\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.env = env\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        \"\"\"\n",
    "        Select an action for the given state using the epsilon-greedy policy.\n",
    "\n",
    "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
    "        Otherwise, the action with the highest estimated action value is chosen.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The current state of the environment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.int64\n",
    "            The chosen action.\n",
    "        \"\"\"\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "\n",
    "                ### BEGIN SOLUTION ###\n",
    "\n",
    "                q_values = ...\n",
    "                action = ...\n",
    "\n",
    "                ### END SOLUTION ###\n",
    "\n",
    "        return action\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"\n",
    "        Decay the epsilon value after each episode.\n",
    "\n",
    "        The new epsilon value is the maximum of `epsilon_min` and the product of the current \n",
    "        epsilon value and `epsilon_decay`.\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing a Learning Rate Scheduler\n",
    "\n",
    "The following cell introduces a PyTorch Learning Rate (LR) scheduler. This scheduler is used for managing and adjusting the learning rate throughout the training process of our agent. It's designed to adjust the learning rate of an optimizer at each epoch, following an exponential decay strategy, but with a lower limit on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of MinimumExponentialLR.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        optimizer : torch.optim.Optimizer\n",
    "            The optimizer whose learning rate should be scheduled.\n",
    "        lr_decay : float\n",
    "            The multiplicative factor of learning rate decay.\n",
    "        last_epoch : int, optional\n",
    "            The index of the last epoch. Default is -1.\n",
    "        min_lr : float, optional\n",
    "            The minimum learning rate. Default is 1e-6.\n",
    "        \"\"\"\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "        \"\"\"\n",
    "        Compute learning rate using chainable form of the scheduler.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[float]\n",
    "            The learning rates of each parameter group.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the Training Function\n",
    "\n",
    "The following function is the final component of our initial agent. It orchestrates the training process, enabling the agent to learn from its interactions with the environment.\n",
    "\n",
    "During each episode, the agent selects actions based on an epsilon-greedy policy, observes the next state and reward from the environment, and updates the weights of the Q-Network based on the observed reward and the maximum predicted Q-value of the next state.\n",
    "\n",
    "**Task 2.5:** complete this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_naive_agent(env: gym.Env,\n",
    "                      q_network: torch.nn.Module,\n",
    "                      optimizer: torch.optim.Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      epsilon_greedy: EpsilonGreedy,\n",
    "                      device: torch.device,\n",
    "                      lr_scheduler: _LRScheduler,\n",
    "                      num_episodes: int,\n",
    "                      gamma: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights\n",
    "\n",
    "            ### BEGIN SOLUTION ###\n",
    "\n",
    "            target = ...\n",
    "            q_value_of_current_action = ...\n",
    "\n",
    "            ### END SOLUTION ###\n",
    "\n",
    "            loss = loss_fn(q_value_of_current_action, target)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "naive_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_naive_agent(env,\n",
    "                                            q_network,\n",
    "                                            optimizer,\n",
    "                                            loss_fn,\n",
    "                                            epsilon_greedy,\n",
    "                                            device,\n",
    "                                            lr_scheduler,\n",
    "                                            num_episodes=150,\n",
    "                                            gamma=0.9)\n",
    "    naive_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    naive_trains_result_list[1].extend(episode_reward_list)\n",
    "    naive_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "naive_trains_result_df = pd.DataFrame(np.array(naive_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "naive_trains_result_df[\"agent\"] = \"Naive\"\n",
    "\n",
    "# Save the action-value estimation function of the last train\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"naive_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=naive_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", hue=\"agent\", kind=\"line\", data=naive_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"naive_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=5)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab6_dqn_naive_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why It Doesn't Work: The Complexity of Deep Reinforcement Learning\n",
    "\n",
    "Our initial deep value-based agent did not converge, primarily due to the three fundamental challenges of value-based deep reinforcement learning:\n",
    "\n",
    "1. **Coverage**: Convergence to the optimal Q-function relies on comprehensive coverage of the state space. However, in the context of deep RL, the state space is often too large to be fully covered. In situations where not all states are sampled due to their vast number, the guarantee of convergence no longer holds.\n",
    "\n",
    "2. **Correlation**: The probability of transitioning to the next state is highly influenced by the current state. This strong correlation can lead to local overfitting and the risk of becoming trapped in a local optimum: the neural network, which approximates the Q-function, may become overly specialized in a small portion of the action-state space and neglect the rest.\n",
    "\n",
    "3. **Convergence**: The \"targets\" used as the truth to be achieved \"move\" during the learning process. For the same prediction (estimation of the value of a state-action pair, i.e., its Q-value), the loss of a given example changes during the learning process (due to *bootstrapping* a main concept of TD-Learning). In other words, DQN tries to minimize a moving target, a target that depends on the model we are learning and optimizing. This can lead to instability and make it difficult for the learning process to converge to an optimal policy.\n",
    "\n",
    "In the following sections, we will explore strategies to address these challenges and improve the performance of our deep reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Implement Deep Q-Networks v1 (DQN version 2013 with experience replay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2013, DeepMind made a significant contribution to the field of reinforcement learning with the publication of the paper \"Playing Atari with Deep Reinforcement Learning\" by Volodymyr Mnih and al (https://arxiv.org/abs/1312.5602). This paper marked the introduction of the first version of Deep Q-Networks (DQN).\n",
    "\n",
    "The paper's primary innovation was the development of a technique to decorrelate states in reinforcement learning. This technique, known as *experience replay*, leverages a *replay buffer* to store and sample experiences. The introduction of experience replay greatly enhanced the stability and efficiency of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Experience replay\n",
    "\n",
    "Experience replay is a key technique used in Deep Q-Networks (DQN) to address the issues of correlation.\n",
    "\n",
    "In a typical reinforcement learning setup, an agent learns by interacting with the environment, receiving feedback in the form of rewards, and updating its policy based on this feedback. This process is inherently sequential and the successive states are highly correlated, which can lead to overfitting and instability in learning.\n",
    "\n",
    "Experience replay addresses these issues by storing the agent's experiences, i.e., the tuples of (state, action, reward, next state), in a data structure known as a replay buffer. During the learning process, instead of learning from the most recent experience, the agent randomly samples a batch of experiences from the replay buffer. This random sampling breaks the correlation between successive experiences, leading to more stable and robust learning.\n",
    "\n",
    "Also, by learning from past experiences, the agent can effectively learn from a fixed target, which mitigates the issue of learning from a moving target. This is because the experiences in the replay buffer remain fixed once they are stored, even though the agent's policy continues to evolve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2013 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $\\color{red}{M}$<br>\n",
    "\t$\\quad\\quad$ batch size $\\color{red}{m}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\color{red}{\\mathcal{D}}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{Store transition } (\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'}) \\text{ in } \\mathcal{D}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\color{red}{\\text{IF } \\mathcal{D} \\text{ contains \"enough\" transitions}}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{Sample random batch of transitions } (\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j) \\text{ from } \\mathcal{D} \\text{ with } j=1 \\text{ to } m}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ $\\color{red}{\\text{For each } j}$, set $y_j \\leftarrow \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Replay Buffer\n",
    "\n",
    "To incorporate experience replay into the provided naive deep value-based reinforcement learning agent definition, we need to introduce a memory buffer where experiences are stored, and then update the algorithm to sample a random batch of experiences from this buffer to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    A Replay Buffer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : collections.deque\n",
    "        A double-ended queue where the transitions are stored.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
    "        Add a new transition to the buffer.\n",
    "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "        Sample a batch of transitions from the buffer.\n",
    "    __len__()\n",
    "        Return the current size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initializes a ReplayBuffer instance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of transitions that can be stored in the buffer.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
    "        \"\"\"\n",
    "        Add a new transition to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : np.ndarray\n",
    "            The state vector of the added transition.\n",
    "        action : np.int64\n",
    "            The action of the added transition.\n",
    "        reward : float\n",
    "            The reward of the added transition.\n",
    "        next_state : np.ndarray\n",
    "            The next state vector of the added transition.\n",
    "        done : bool\n",
    "            The final state of the added transition.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of transitions to sample.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
    "            A batch of `batch_size` transitions.\n",
    "        \"\"\"\n",
    "        # Here, `random.sample(self.buffer, batch_size)`\n",
    "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
    "        # where:\n",
    "        # - `state`  and `next_state` are numpy arrays\n",
    "        # - `action` and `reward` are floats\n",
    "        # - `done` is a boolean\n",
    "        #\n",
    "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
    "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "The training function of our initial deep value-based agent needs to be modified to incorporate the use of the replay buffer effectively.\n",
    "\n",
    "1. **Store Experiences**: After the agent takes an action and receives a reward and the next state from the environment, store this experience in the replay buffer.\n",
    "\n",
    "2. **Sample Experiences**: Instead of using the most recent experience to update the agent's policy, randomly sample a batch of experiences from the replay buffer.\n",
    "\n",
    "3. **Compute Loss and Update Weights**: Use the sampled experiences to compute the loss and update the weights of the Q-Network.\n",
    "\n",
    "4. **Handle Terminal States**: If the 'done' flag of an experience is True, indicating a terminal state, make sure to adjust the target Q-value to be just the received reward. This is because there are no future rewards possible after a terminal state.\n",
    "\n",
    "**Task 3.1:** complete the `train_dqn1_agent` to use the replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn1_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                ### BEGIN SOLUTION ###\n",
    "\n",
    "                targets = ...\n",
    "                current_q_values = ...\n",
    "\n",
    "                ### END SOLUTION ###\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
    "dqn1_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "    \n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "    \n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn1_agent(env,\n",
    "                                           q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer)\n",
    "    dqn1_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn1_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn1_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn1_trains_result_df = pd.DataFrame(np.array(dqn1_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn1_trains_result_df[\"agent\"] = \"DQN 2013\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"dqn1_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn1_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn1_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn1_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab6_dqn1_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex3 = dqn1_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Implement Deep Q-Networks v2 (DQN version 2015) with *infrequent weight updates*\n",
    "\n",
    "In 2015, DeepMind further advanced the field of reinforcement learning with the publication of the paper \"Human-level control through deep reinforcement learning\" by Volodymyr Mnih and colleagues (https://www.nature.com/articles/nature14236). This work introduced the second version of Deep Q-Networks (DQN).\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/lab1_dqn_nature_journal.jpg?raw=true\" width=\"200px\" />\n",
    "\n",
    "The key contribution of this paper was the introduction of a method to stabilize the learning process by infrequently updating the target weights. This technique, known as *infrequent updates of target weights*, significantly improved the stability of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infrequent weight updates\n",
    "\n",
    "Infrequent weight updates, also known as the use of a target network, is a technique used in Deep Q-Networks (DQN) to address the issue of learning from a moving target.\n",
    "\n",
    "In a typical DQN setup, there are two neural networks: the Q-network and the target network. The Q-network is used to predict the Q-values and is updated at every time step. The target network is used to compute the target Q-values for the update, and its weights are updated less frequently, typically every few thousand steps, by copying the weights from the Q-network.\n",
    "\n",
    "The idea behind infrequent weight updates is to stabilize the learning process by keeping the target Q-values fixed for a number of steps. This mitigates the issue of learning from a moving target, as the target Q-values remain fixed between updates.\n",
    "\n",
    "Without infrequent weight updates, both the predicted and target Q-values would change at every step, which could lead to oscillations and divergence in the learning process. By introducing a delay between updates of the target Q-values, the risk of such oscillations is reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN v2015 Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: main differences with the previous algorithm are highlighted in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\color{red}{\\tau}$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}_{\\mathbf{\\omega_1}}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}_{\\mathbf{\\omega_2}}$ with weights $\\color{red}{\\mathbf{\\omega_2} = \\mathbf{\\omega_1}}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j = \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\max_{\\mathbf{a}^\\star} \\hat{Q}_{\\mathbf{\\omega_{\\color{red}{2}}}} (\\mathbf{s'}_j)_{\\mathbf{a}^\\star} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\color{red}{\\tau}$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\color{red}{\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "To incorporate the use of infrequent weight updates in the training function, you would need to make the following modifications:\n",
    "\n",
    "1. **Update the Target Network Infrequently**: Instead of updating the weights of the target network at every time step, update them less frequently, for example, every few thousand steps. The weights of the target network are updated by copying the weights from the Q-network.\n",
    "\n",
    "2. **Compute Target Q-values with the Target Network**: When computing the target Q-values for the update, use the target network instead of the Q-network. This ensures that the target Q-values remain fixed between updates, which stabilizes the learning process.\n",
    "\n",
    "**Task 4.1:** complete the `train_dqn2_agent` to apply infrequent weight updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                ### BEGIN SOLUTION ###\n",
    "\n",
    "                targets = ...\n",
    "                current_q_values = ...\n",
    "\n",
    "                ### END SOLUTION ###\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update the target q-network\n",
    "\n",
    "            ### BEGIN SOLUTION ###\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            ### END SOLUTION ###\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train it\n",
    "\n",
    "In order to test this new implementation, we needs de adapt the following cell to instantiate and initialize the two neural networks.\n",
    "\n",
    "**Task 4.2:** complete the following cell to make the two Q-Networks. Initialize a target network that has the same architecture as the Q-network. The weights of the target network are initially copied from the Q-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# NUMBER_OF_TRAININGS = 20\n",
    "dqn2_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    q_network = ...\n",
    "    target_q_network = ...\n",
    "    # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(env,\n",
    "                                           q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(np.array(dqn2_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"dqn2_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn2_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn2_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn2_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab6_dqn2_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_ex4 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
    "score_ex4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Double Deep Q-Network (DDQN)\n",
    "\n",
    "Hado Van Hasselt et al. introduced Double Deep Q-Networks in the publication \"Deep reinforcement learning with Double Q-Learning\" in 2016 (https://arxiv.org/abs/1509.06461).\n",
    "\n",
    "Double Deep Q-Networks (DDQN) is an enhancement over the standard Deep Q-Network (DQN). It was designed to reduce the overestimation of action values that can occur in DQN. The fundamental concept behind DDQN is the separation of action selection from their evaluation to reduce the overestimation of action values in DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DDQN Algorithm\n",
    "\n",
    "Note: main differences with the previous algorithm are highlighted in red.\n",
    "\n",
    "<b>Input</b>:<br>\n",
    "\t$\\quad\\quad$ none<br>\n",
    "<b>Algorithm parameter</b>:<br>\n",
    "\t$\\quad\\quad$ discount factor $\\gamma$<br>\n",
    "\t$\\quad\\quad$ step size $\\alpha \\in (0,1]$<br>\n",
    "\t$\\quad\\quad$ small $\\epsilon > 0$<br>\n",
    "\t$\\quad\\quad$ capacity of the experience replay memory $M$<br>\n",
    "\t$\\quad\\quad$ batch size $m$<br>\n",
    "\t$\\quad\\quad$ target network update frequency $\\tau$<br><br>\n",
    "\n",
    "<b>Initialize</b> replay memory $\\mathcal{D}$ to capacity $M$<br>\n",
    "<b>Initialize</b> action-value function $\\hat{Q}$ with random weights $\\mathbf{\\omega_1}$<br>\n",
    "<b>Initialize</b> target action-value function $\\hat{Q}$ with weights $\\mathbf{\\omega_2} = \\mathbf{\\omega_1}$<br><br>\n",
    "\n",
    "<b>FOR EACH</b> episode<br>\n",
    "\t$\\quad$ $\\mathbf{s} \\leftarrow \\text{env.reset}()$<br>\n",
    "\t$\\quad$ <b>DO</b> <br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{a} \\leftarrow \\epsilon\\text{-greedy}(\\mathbf{s}, \\hat{Q}_{\\mathbf{\\omega_1}})$<br>\n",
    "\t\t$\\quad\\quad$ $r, \\mathbf{s'} \\leftarrow \\text{env.step}(\\mathbf{a})$<br>\n",
    "\t\t$\\quad\\quad$ Store transition $(\\mathbf{s}, \\mathbf{a}, r, \\mathbf{s'})$ in $\\mathcal{D}$<br>\n",
    "\t\t$\\quad\\quad$ If $\\mathcal{D}$ contains \"enough\" transitions<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Sample random batch of transitions $(\\mathbf{s}_j, \\mathbf{a}_j, r_j, \\mathbf{s'}_j)$ from $\\mathcal{D}$ with $j=1$ to $m$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $\\color{red}{\\mathbf{a}^{\\star} \\leftarrow \\arg\\max_{\\mathbf{a}} \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s'}_j)_{\\mathbf{a}}}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ For each $j$, set $y_j \\leftarrow \n",
    "\t\t\t\\begin{cases} \n",
    "\t\t\tr_j & \\text{for terminal } \\mathbf{s'}_j\\\\\n",
    "\t\t\tr_j + \\gamma \\hat{Q}_{\\mathbf{\\omega_2}} (\\mathbf{s'}_j)_{\\color{red}{\\mathbf{a}^\\star}} & \\text{for non-terminal } \\mathbf{s'}_j\n",
    "\t\t\t\\end{cases}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Perform a gradient descent step on $\\left( y_j - \\hat{Q}_{\\mathbf{\\omega_1}}(\\mathbf{s}_j)_{\\mathbf{a}_j} \\right)^2$ with respect to the weights $\\mathbf{\\omega_1}$<br>\n",
    "\t\t\t$\\quad\\quad\\quad$ Every $\\tau$ steps reset $\\hat{Q}_{\\mathbf{\\omega_2}}$ to $\\hat{Q}_{\\mathbf{\\omega_1}}$, i.e., set $\\mathbf{\\omega_2} \\leftarrow \\mathbf{\\omega_1}$<br>\n",
    "\t\t$\\quad\\quad$ $\\mathbf{s} \\leftarrow \\mathbf{s'}$ <br>\n",
    "\t$\\quad$ <b>UNTIL</b> $\\mathbf{s}$ is final<br><br>\n",
    "<b>RETURN</b> $\\mathbf{\\omega_1}$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Implement DDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "Switching from a Deep Q-Network (DQN) to a Double Deep Q-Network (DDQN) involves a key modification in the way the Q-value update is performed during training. \n",
    "\n",
    "In DQN, the Q-value update is done using the maximum Q-value for the next state from the target network. However, this can lead to an overestimation of Q-values because it always uses the maximum estimate.\n",
    "\n",
    "DDQN addresses this by decoupling the selection of the action from the evaluation of that action. In DDQN, the Q-network is used to select what the next action is, and the target network is used to evaluate the Q-value of taking that action at the next state.\n",
    "\n",
    "**Task 4.2:** complete the `train_ddqn_agent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ddqn_agent(env: gym.Env,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    ### BEGIN SOLUTION ###\n",
    "\n",
    "                    targets = ...\n",
    "\n",
    "                    ### END SOLUTION ###\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "ddqn_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_ddqn_agent(env,\n",
    "                                           q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    ddqn_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    ddqn_trains_result_list[1].extend(episode_reward_list)\n",
    "    ddqn_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "ddqn_trains_result_df = pd.DataFrame(np.array(ddqn_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "ddqn_trains_result_df[\"agent\"] = \"DDQN\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"ddqn_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=ddqn_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=ddqn_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"ddqn_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ddqn_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental results\n",
    "\n",
    "**Task 5.2:** What do you observe? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Prioritized Experience Replay (PEX or PER)\n",
    "\n",
    "Prioritized Experience Replay (PER) is an enhancement to the traditional experience replay mechanism used in Deep Q-Networks (DQNs) and other reinforcement learning algorithms. In traditional experience replay, experiences (or transitions) are stored in a replay buffer, and mini-batches are sampled uniformly at random from this buffer to update the agent's Q-network. This means every experience has the same probability of being chosen, regardless of its significance to the learning process.\n",
    "\n",
    "Prioritized Experience Replay (PER) is a technique that modifies the standard experience replay by more frequently replaying experiences that have a high expected learning progress, as measured by their temporal-difference (TD) error.\n",
    "\n",
    "Prioritized Experience Replay was introduced by Tom Schaul et al. in the paper \"Prioritised experience replay\" in 2016 (https://arxiv.org/abs/1511.05952)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DQN with PER algorithm\n",
    "\n",
    "The DQN with Prioritized Experience Replay (PER) algorithm is detailed in page 5 of the paper \"Prioritised experience replay\" (https://arxiv.org/abs/1511.05952)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Implement DQN with PER\n",
    "\n",
    "To switch from a DQN to a DQN with Prioritized Experience Replay (PER), you need to make a few modifications to the *DQN 2015* agent:\n",
    "\n",
    "1. **Experience Replay Buffer**: Replace the standard experience replay buffer with a prioritized one. This buffer should store experiences with a priority value that is updated after each learning step. The priority value is usually the absolute TD error plus a small constant to avoid experiences having zero probability of being chosen.\n",
    "2. **Sampling Method**: Change the sampling method from uniform to prioritized sampling. This means experiences with higher priority values have a higher probability of being chosen for learning.\n",
    "3. **Loss Function**: Modify the loss function to include importance sampling weights. These weights compensate for the bias introduced by the non-uniform sampling. The weight of each sampled experience is the inverse of its probability of being chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the PrioritizedReplayBuffer\n",
    "\n",
    "**Task 6.1:** implement the `PrioritizedReplayBuffer` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Implements a Prioritized Experience Replay buffer as described in the paper\n",
    "    \"Prioritized Experience Replay\" (https://arxiv.org/abs/1511.05952).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    buffer : Deque[Tuple[float, float, float, float, float]]\n",
    "        The replay buffer storing the experiences.\n",
    "    priorities : Deque[float]\n",
    "        The priorities of the experiences in the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"\n",
    "        Initialize the replay buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        capacity : int\n",
    "            The maximum number of experiences the buffer can hold.\n",
    "        \"\"\"\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "        self.priorities = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: float, action: float, reward: float, next_state: float, done: float):\n",
    "        \"\"\"\n",
    "        Add an experience to the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : float\n",
    "            The current state.\n",
    "        action : float\n",
    "            The action taken.\n",
    "        reward : float\n",
    "            The reward received.\n",
    "        next_state : float\n",
    "            The next state.\n",
    "        done : float\n",
    "            Whether the episode has ended.\n",
    "        \"\"\"\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(max(self.priorities, default=1))\n",
    "\n",
    "    def sample(self, batch_size: int, alpha: float = 0.6, beta: float = 0.4) -> Tuple[List[Tuple[float, float, float, float, float]], np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample a batch of experiences from the buffer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            The number of experiences to sample.\n",
    "        alpha : float, optional\n",
    "            The exponent that determines how much prioritization is used.\n",
    "        beta : float, optional\n",
    "            The exponent that determines how much importance sampling is used.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[List[Tuple[float, float, float, float, float]], np.ndarray, np.ndarray]\n",
    "            The sampled experiences, the indices of the sampled experiences, and the importance sampling weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        # Calculate priorities\n",
    "        priorities = ...\n",
    "\n",
    "        # Sample experiences\n",
    "        indices = ...\n",
    "        experiences = ...\n",
    "\n",
    "        # Calculate weights\n",
    "        weights = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return experiences, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices: np.ndarray, errors: List[float], offset: float = 0.1):\n",
    "        \"\"\"\n",
    "        Update the priorities of the sampled experiences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        indices : np.ndarray\n",
    "            The indices of the sampled experiences.\n",
    "        errors : List[float]\n",
    "            The new TD errors for the sampled experiences.\n",
    "        offset : float, optional\n",
    "            A small constant to ensure the priorities are strictly positive.\n",
    "        \"\"\"\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = error + offset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the current size of the buffer.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current size of the buffer.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "**Task 6.2:** complete the `train_dqn_per_agent` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_per_agent(env: gym.Env,\n",
    "                        q_network: torch.nn.Module,\n",
    "                        target_q_network: torch.nn.Module,\n",
    "                        optimizer: torch.optim.Optimizer,\n",
    "                        loss_fn: Callable,\n",
    "                        epsilon_greedy: EpsilonGreedy,\n",
    "                        device: torch.device,\n",
    "                        lr_scheduler: _LRScheduler,\n",
    "                        num_episodes: int,\n",
    "                        gamma: float,\n",
    "                        batch_size: int,\n",
    "                        replay_buffer: PrioritizedReplayBuffer,  # Use a Prioritized Replay Buffer\n",
    "                        target_q_network_sync_period: int,\n",
    "                        alpha: float = 0.6,  # Alpha parameter for PER\n",
    "                        beta: float = 0.4):  # Beta parameter for PER\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : PrioritizedReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "    alpha : float\n",
    "        The exponent that determines how much prioritization is used when sampling from the replay buffer.\n",
    "    beta : float\n",
    "        The exponent that determines how much importance sampling is used to adjust the loss function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                # Sample experiences and importance-sampling weights from the buffer\n",
    "\n",
    "                experiences, indices, weights = replay_buffer.sample(batch_size, alpha, beta)\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "                weights_tensor = torch.tensor(weights, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "                    # Here's a breakdown of the next line of code:\n",
    "                    # - `q_network(batch_next_states_t)`:\n",
    "                    #   This is passing a batch of \"next states\" through the Q-network.\n",
    "                    #   This outputs the Q-value for each possible action, a tensor of shape (batch_size, action_dim).\n",
    "                    #\n",
    "                    #  - `.max(dim=1)`:\n",
    "                    #   This is finding the maximum Q-value for each state in the batch.\n",
    "                    #   The dim=1 argument specifies that the maximum should be taken over the action dimension.\n",
    "                    #\n",
    "                    # The max() function in PyTorch returns a tuple containing two tensors: the maximum values and the indices where these maximum values were found.\n",
    "                    # In the next lines of code, we will just use the first tensor (the maximum values) and ignoring the second tensor (the indices).\n",
    "                    next_state_q_values, best_action_index = target_q_network(batch_next_states_tensor).max(dim=1)\n",
    "\n",
    "                    # The targets for the batch are the rewards plus the discounted maximum Q-values obtained from the next states.\n",
    "                    # The expression `(1 - batch_dones_tensor)` is used to handle the end of episodes.\n",
    "                    # The `batch_dones_tensor` indicates whether each state in the batch is a terminal state (i.e., the end of an episode).\n",
    "                    # If a state is a terminal state, the corresponding value in `batch_dones_tensor` is 1, otherwise it's 0.\n",
    "                    # The Q-value of a terminal state is defined to be 0. Therefore, when calculating the target Q-values,\n",
    "                    # we don't want to include the Q-value of the next state if the current state is a terminal state.\n",
    "                    # This is achieved by multiplying `next_state_q_values` by `(1 - batch_dones_tensor)`.\n",
    "                    # If the state is a terminal state, this expression becomes 0 and the Q-value of the next state is effectively ignored.\n",
    "                    # If the state is not a terminal state, this expression is 1 and the Q-value of the next state is included in the calculation.\n",
    "                    targets = batch_rewards_tensor + gamma * next_state_q_values * (1 - batch_dones_tensor)\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss with importance-sampling weights\n",
    "\n",
    "                ### BEGIN SOLUTION ###\n",
    "\n",
    "                loss = ...\n",
    "\n",
    "                ### END SOLUTION ###\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "\n",
    "                ### BEGIN SOLUTION ###\n",
    "\n",
    "                # Update priorities in the buffer\n",
    "\n",
    "                ### END SOLUTION ###\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "dqn_per_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # INSTANTIATE REQUIRED OBJECTS\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = QNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = PrioritizedReplayBuffer(2000)\n",
    "\n",
    "    # TRAIN THE Q-NETWORK\n",
    "\n",
    "    episode_reward_list = train_dqn_per_agent(env,\n",
    "                                              q_network,\n",
    "                                              target_q_network,\n",
    "                                              optimizer,\n",
    "                                              loss_fn,\n",
    "                                              epsilon_greedy,\n",
    "                                              device,\n",
    "                                              lr_scheduler,\n",
    "                                              num_episodes=150,\n",
    "                                              gamma=0.9,\n",
    "                                              batch_size=128,\n",
    "                                              replay_buffer=replay_buffer,\n",
    "                                              target_q_network_sync_period=30,\n",
    "                                              alpha=0.6,\n",
    "                                              beta=0.4)\n",
    "    dqn_per_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn_per_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn_per_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn_per_trains_result_df = pd.DataFrame(np.array(dqn_per_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn_per_trains_result_df[\"agent\"] = \"DQN 2015 + PER\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"dqn_per_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dqn_per_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dqn_per_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df, dqn_per_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dqn_per_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dqn_per_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Experimental results\n",
    "\n",
    "**Task 6.2:** What do you observe? Why?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Dueling Double DQN : DDQN with Advantage function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dueling Double DQN (Dueling DDQN) is an enhancement over the standard DQN that aims to improve the quality of the learned value function (https://arxiv.org/abs/1511.06581). The motivation for Dueling DDQN lies in its architecture, which separately estimates two components: the value of being in a particular state (V(s)), and the advantage of taking a particular action in that state (A(s, a)).\n",
    "\n",
    "In standard DQN, a single stream of network layers estimates the Q-value directly. In contrast, Dueling DDQN has two streams to separately estimate the value and advantage functions, which are then combined to calculate the Q-value. This allows the network to more effectively learn which states are (or are not) valuable without having to learn the effect of each action for each state. This is particularly useful in environments where the value of the state does not vary much across actions.\n",
    "\n",
    "The separation of the estimation process helps in stabilizing learning and often leads to better policy evaluation, especially in cases where the action choice does not have a large impact on what happens next—making Dueling DDQN a more robust and often more efficient learning algorithm compared to the standard DQN.\n",
    "\n",
    "Thus, the Dueling DQN architecture introduces two separate streams within the neural network:\n",
    "\n",
    "   - **Value Stream (`V(s)`):** Estimates the overall value of being in a state, regardless of the action taken.\n",
    "   - **Advantage Stream (`A(s, a)`):** Estimates the relative advantage of each action in a given state.\n",
    "\n",
    "These two streams share some common layers at the beginning and then branch out. They are combined at the end to produce the Q-values using the following formula:\n",
    "\n",
    "$$Q(s, a) = V(s) + \\left( A(s, a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s, a') \\right)$$\n",
    "\n",
    "where:\n",
    "- $A(s, a)$ is the advantage of action $a$ in state $s$.\n",
    "- $|\\mathcal{A}|$ is the number of possible actions.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/336715783/figure/fig2/AS:816675293253632@1571721971001/Double-Dueling-Deep-Q-Learning-Network-DDDQN.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: Implement DDDQN\n",
    "\n",
    "Modify the above `QNetwork` class to implement a **Dueling Double Deep Q-Network (DDDQN)** by completing the `__init__` and `forward` methods of the new class `DuelingQNetwork` partially implemented below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Dueling DQN network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the Q-Network\n",
    "\n",
    "**Task 7.1:** complete the `DuelingDQN` class.\n",
    "\n",
    "- **In the `__init__` Method:**\n",
    "  - Initialize the shared layers (`layer1` and `layer2`) as in the original `QNetwork`.\n",
    "  - Define two separate streams after the shared layers:\n",
    "    - **Value Stream:**\n",
    "      - A fully connected layer (`self.value_layer`) that outputs a single value.\n",
    "    - **Advantage Stream:**\n",
    "      - A fully connected layer (`self.advantage_layer`) that outputs a value for each possible action.\n",
    "\n",
    "- **In the `forward` Method:**\n",
    "  - Pass the input through the shared layers with activation functions.\n",
    "  - Pass the output of the shared layers into both the value and advantage streams.\n",
    "  - Combine the outputs of the value and advantage streams to compute the final Q-values using the formula provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingQNetwork(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A Dueling Q-Network implemented with PyTorch.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    layer1 : torch.nn.Linear\n",
    "        First fully connected layer (common feature layer).\n",
    "    layer2 : torch.nn.Linear\n",
    "        Second fully connected layer (common feature layer).\n",
    "    value_layer : torch.nn.Linear\n",
    "        Linear layer for the value stream.\n",
    "    advantage_layer : torch.nn.Linear\n",
    "        Linear layer for the advantage stream.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    forward(x: torch.Tensor) -> torch.Tensor\n",
    "        Define the forward pass of the DuelingQNetwork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations: int, n_actions: int, nn_l1: int, nn_l2: int):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of DuelingQNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_observations : int\n",
    "            The size of the observation space.\n",
    "        n_actions : int\n",
    "            The size of the action space.\n",
    "        nn_l1 : int\n",
    "            The number of neurons in the first hidden layer.\n",
    "        nn_l2 : int\n",
    "            The number of neurons in the second hidden layer.\n",
    "        \"\"\"\n",
    "        super(DuelingQNetwork, self).__init__()\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Define the forward pass of the DuelingQNetwork.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : torch.Tensor\n",
    "            The input tensor (state).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            The output tensor (Q-values).\n",
    "        \"\"\"\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "        \n",
    "        ...\n",
    "\n",
    "        q_values = ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement the training function\n",
    "\n",
    "**Task 7.2:** complete the `train_dddqn_agent` function (you can simply copy / paste your response from `train_ddqn_agent`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dddqn_agent(env: gym.Env,\n",
    "                      q_network: torch.nn.Module,\n",
    "                      target_q_network: torch.nn.Module,\n",
    "                      optimizer: torch.optim.Optimizer,\n",
    "                      loss_fn: Callable,\n",
    "                      epsilon_greedy: EpsilonGreedy,\n",
    "                      device: torch.device,\n",
    "                      lr_scheduler: _LRScheduler,\n",
    "                      num_episodes: int,\n",
    "                      gamma: float,\n",
    "                      batch_size: int,\n",
    "                      replay_buffer: ReplayBuffer,\n",
    "                      target_q_network_sync_period: int) -> List[float]:\n",
    "    \"\"\"\n",
    "    Train the Q-network on the given environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env : gym.Env\n",
    "        The environment to train on.\n",
    "    q_network : torch.nn.Module\n",
    "        The Q-network to train.\n",
    "    target_q_network : torch.nn.Module\n",
    "        The target Q-network to use for estimating the target Q-values.\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        The optimizer to use for training.\n",
    "    loss_fn : callable\n",
    "        The loss function to use for training.\n",
    "    epsilon_greedy : EpsilonGreedy\n",
    "        The epsilon-greedy policy to use for action selection.\n",
    "    device : torch.device\n",
    "        The device to use for PyTorch computations.\n",
    "    lr_scheduler : torch.optim.lr_scheduler._LRScheduler\n",
    "        The learning rate scheduler to adjust the learning rate during training.\n",
    "    num_episodes : int\n",
    "        The number of episodes to train for.\n",
    "    gamma : float\n",
    "        The discount factor for future rewards.\n",
    "    batch_size : int\n",
    "        The size of the batch to use for training.\n",
    "    replay_buffer : ReplayBuffer\n",
    "        The replay buffer storing the experiences with their priorities.\n",
    "    target_q_network_sync_period : int\n",
    "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[float]\n",
    "        A list of cumulated rewards per episode.\n",
    "    \"\"\"\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, info = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # GET ACTION, NEXT_STATE AND REWARD ###########\n",
    "\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # UPDATE THE Q_NETWORK WEIGHTS WITH A BATCH OF EXPERIENCES FROM THE BUFFER\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                # Compute the target Q values for the batch\n",
    "                with torch.no_grad():\n",
    "\n",
    "                    ### BEGIN SOLUTION ###\n",
    "\n",
    "                    targets = ...\n",
    "\n",
    "                    ### END SOLUTION ###\n",
    "\n",
    "                # Compute the current Q values for the batch.\n",
    "                # \n",
    "                # The expression `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)` is used to select specific elements from the tensor of Q-values returned by the Q-network.\n",
    "                # \n",
    "                # Here's a breakdown of the following line of code:\n",
    "                # - `q_network(batch_states_tensor)`:\n",
    "                #   This is passing a batch of states through the Q-network.\n",
    "                #   For each state, this outputs the Q-value for each possible action.\n",
    "                #   Thus, `q_network(batch_states_tensor)` returns a tensor of shape (batch_size, action_dim).\n",
    "                # \n",
    "                # - `gather(dim=1, index=batch_actions_tensor.unsqueeze(-1))`:\n",
    "                #   This is selecting the Q-values corresponding to the actions that were actually taken.\n",
    "                #   The `gather` function is used to select elements from a tensor using an index.\n",
    "                #   In this case, the index is `batch_actions_tensor.unsqueeze(-1)`, which is a tensor of the actions that were taken.\n",
    "                #   The `unsqueeze(-1)` function is used to add an extra dimension to the tensor, which is necessary for the `gather` function.\n",
    "                # \n",
    "                # - `squeeze(-1)`:\n",
    "                #   This is removing the extra dimension that was added by `unsqueeze(-1)`.\n",
    "                #   The `squeeze` function is used to remove dimensions of size 1 from a tensor.\n",
    "                #\n",
    "                # So, the entire expression is selecting the Q-values of the actions that were actually taken from the tensor of all Q-values,\n",
    "                # and returning a tensor of these selected Q-values.\n",
    "                current_q_values = q_network(batch_states_tensor).gather(dim=1, index=batch_actions_tensor.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = loss_fn(current_q_values, targets)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # UPDATE THE TARGET Q-NETWORK #################\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS    # Change the default (global) value here if you want a specific number of trainings for this exercice\n",
    "dddqn_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "\n",
    "    # Instantiate required objects\n",
    "\n",
    "    q_network = DuelingQNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device)\n",
    "    target_q_network = DuelingQNetwork(state_dim, action_dim, nn_l1=128, nn_l2=128).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # Initialize the target Q-network with the same weights as the Q-network\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, env=env, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dddqn_agent(env,\n",
    "                                            q_network,\n",
    "                                            target_q_network,\n",
    "                                            optimizer,\n",
    "                                            loss_fn,\n",
    "                                            epsilon_greedy,\n",
    "                                            device,\n",
    "                                            lr_scheduler,\n",
    "                                            num_episodes=150,\n",
    "                                            gamma=0.9,\n",
    "                                            batch_size=128,\n",
    "                                            replay_buffer=replay_buffer,\n",
    "                                            target_q_network_sync_period=30)\n",
    "    dddqn_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dddqn_trains_result_list[1].extend(episode_reward_list)\n",
    "    dddqn_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dddqn_trains_result_df = pd.DataFrame(np.array(dddqn_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dddqn_trains_result_df[\"agent\"] = \"DDDQN\"\n",
    "\n",
    "# SAVE THE ACTION-VALUE ESTIMATION FUNCTION\n",
    "\n",
    "torch.save(q_network, MODELS_DIR / \"dddqn_q_network.pth\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", estimator=None, units=\"training_index\", data=dddqn_trains_result_df,\n",
    "                height=7, aspect=2, alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=dddqn_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trains_result_df = pd.concat([naive_trains_result_df, dqn1_trains_result_df, dqn2_trains_result_df, ddqn_trains_result_df, dqn_per_trains_result_df, dddqn_trains_result_df])\n",
    "g = sns.relplot(x=\"num_episodes\", y=\"mean_final_episode_reward\", kind=\"line\", hue=\"agent\", data=all_trains_result_df, height=7, aspect=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q_network = torch.load(\"dddqn_q_network.pth\").to(device)\n",
    "\n",
    "env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "test_q_network_agent(env, q_network, num_episode=3)\n",
    "\n",
    "env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_dddqn_tained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Hyperparameters optimization with Optuna\n",
    "\n",
    "Optuna is an open-source hyperparameter optimization framework designed to automate the process of searching for the best hyperparameters in machine learning models. It is highly efficient and flexible, supporting various optimization algorithms. Optuna works with Python-based machine learning libraries like PyTorch, TensorFlow, and Scikit-learn. Optuna’s core feature is its ability to perform dynamic search spaces and pruning, allowing faster convergence by terminating poorly performing trials early.\n",
    "Optuna supports distributed optimization for large-scale tuning.\n",
    "\n",
    "### Official documentation\n",
    "\n",
    "- Optuna GitHub: [https://github.com/optuna/optuna](https://github.com/optuna/optuna)\n",
    "- Optuna Documentation: [https://optuna.org](https://optuna.org)\n",
    "\n",
    "### Example of usage with PyTorch\n",
    "\n",
    "Here's an example of how to use Optuna to optimize the hyperparameters of a simple neural network with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the PyTorch model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be tuned\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 128)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "    \n",
    "    model = Net(input_size=28*28, hidden_size=hidden_size, output_size=10)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Dummy dataset\n",
    "    X = torch.randn(100, 28*28)\n",
    "    y = torch.randint(0, 10, (100,))\n",
    "    train_loader = DataLoader(TensorDataset(X, y), batch_size=32)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_x)\n",
    "            loss = loss_fn(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Optimize hyperparameters\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Show best hyperparameters\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example creates a basic neural network and tunes the `hidden_size` and learning rate (`lr`) using Optuna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 3: Monitoring the training process with *Weights & Biases*\n",
    "\n",
    "Weights & Biases (W&B) is designed for machine learning practitioners to track and visualize experiments, making it an alternative to TensorBoard.\n",
    "It integrates with frameworks like PyTorch, helping users monitor model training, compare hyperparameters, visualize performance metrics, and share results.\n",
    "\n",
    "To get started, you'll need to [create a free account on the W&B website](https://wandb.ai/site). Once registered, a unique token will be generated to authenticate your account. This token is used with the `wandb.login()` function to log in and link your experiments to the W&B dashboard.\n",
    "\n",
    "Below is a simple usage example, sourced from [this Colab notebook](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/intro/Intro_to_Weights_%26_Biases.ipynb#scrollTo=1UU7kVQF2lOJ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import wandb\n",
    "import random\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "# Launch 5 simulated experiments\n",
    "total_runs = 5\n",
    "for run in range(total_runs):\n",
    "  # 1️. Start a new run to track this script\n",
    "  wandb.init(\n",
    "      # Set the project where this run will be logged\n",
    "      project=\"basic-intro\",\n",
    "      # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
    "      name=f\"experiment_{run}\",\n",
    "      # Track hyperparameters and run metadata\n",
    "      config={\n",
    "          \"learning_rate\": 0.02,\n",
    "          \"architecture\": \"CNN\",\n",
    "          \"dataset\": \"CIFAR-100\",\n",
    "          \"epochs\": 10,\n",
    "      })\n",
    "\n",
    "  # This simple block simulates a training loop logging metrics\n",
    "  epochs = 10\n",
    "  offset = random.random() / 5\n",
    "  for epoch in range(2, epochs):\n",
    "      acc = 1 - 2 ** -epoch - random.random() / epoch - offset\n",
    "      loss = 2 ** -epoch + random.random() / epoch + offset\n",
    "\n",
    "      # 2️. Log metrics from your script to W&B\n",
    "      wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "  # Mark the run as finished\n",
    "  wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the training process in the Weights & Biases dashboard at https://wandb.ai/site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize a new W&B run, use `wandb.init()`:\n",
    "\n",
    "```python\n",
    "run = wandb.init(project=\"my-model-training-project\")\n",
    "```\n",
    "\n",
    "You can log hyperparameters by setting `run.config = {...}`:\n",
    "\n",
    "```python\n",
    "run.config.update({\"epochs\": 1337, \"learning_rate\": 3e-4})\n",
    "```\n",
    "\n",
    "To track metrics, use the `run.log()` method:\n",
    "\n",
    "```python\n",
    "run.log({\"metric\": 42})\n",
    "```\n",
    "\n",
    "In addition, you can log artifacts such as models, datasets, and images:\n",
    "\n",
    "```python\n",
    "model_artifact = run.log_artifact(\"./model.pt\", type=\"model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 4: Test and train a DQN agent to play Atari games\n",
    "\n",
    "Training a DQN agent from scratch on the Atari environment can take several hours on a machine with a good GPU. Finding the right hyperparameters requires this training to be repeated many times. However, here you will find a pre-trained agents to test DQN on the Atari environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Except if you have selected the `requirements-minimal.txt` file, atari is already installed. If not, you can install it with the following command:\n",
    "\n",
    "- For Gymnasium  < 1.0.0: `pip install gymnasium[accept-rom-license,atari]`\n",
    "- For Gymnasium >= 1.0.0: `pip install gymnasium[atari]`\n",
    "\n",
    "As described in the following post `gymnasium[atari]` [is not compatible with Python 3.12](https://github.com/Farama-Foundation/Gymnasium/issues/1081) (the latest stable version of Python).\n",
    "If you plan to complete this bonus section locally on your machine rather than on Google Colab, make sure you have Python 3.10 or 3.11 installed.\n",
    "\n",
    "If you are using Python 3.12 and prefer not to run this notebook on Google Colab, an alternative solution via Docker is provided bellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Docker\n",
    "\n",
    "We supose that you have Docker installed on your machine. If not, you can download it [here](https://www.docker.com/products/docker-desktop).\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -v ./:/inf639 inf639:latest python3 ./your_dqn_training_script.py\n",
    "```\n",
    "\n",
    "If you want to monitor the training process with Weights & Biases, you can use the following command, assuming your token is stored in the `WANDB_API_KEY` environment variable:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -h your-hostname -e WANDB_API_KEY=$WANDB_API_KEY -e WANDB_PROJECT_NAME=\"inf639\" -e WANDB_ENTITY=\"your-entity-name\" -v ./:/inf639 inf639:latest python3 ./your_dqn_training_script.py\n",
    "```\n",
    "\n",
    "If you want to use a GPU within a Docker container, you have to install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html) then you can use the following command:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all -it --rm -h your-hostname -e WANDB_API_KEY=$WANDB_API_KEY -e WANDB_PROJECT_NAME=\"inf639\" -e WANDB_ENTITY=\"your-entity-name\" -v ./:/inf639 inf639:latest python3 ./your_dqn_training_script.py\n",
    "```\n",
    "\n",
    "If you have multiple GPUs, you can specify the GPU index with the `--gpus` option. For example, to use the second GPU, you can use the following command to run the training script on the first GPU:\n",
    "\n",
    "```bash\n",
    "docker run --gpus '\"device=0\"' -it --rm -h your-hostname -e WANDB_API_KEY=$WANDB_API_KEY -e WANDB_PROJECT_NAME=\"inf639\" -e WANDB_ENTITY=\"your-entity-name\" -v ./:/inf639 inf639:latest python3 ./your_dqn_training_script.py\n",
    "```\n",
    "\n",
    "and this one to run the training script on the second GPU:\n",
    "\n",
    "```bash\n",
    "docker run --gpus '\"device=1\"' -it --rm -h your-hostname -e WANDB_API_KEY=$WANDB_API_KEY -e WANDB_PROJECT_NAME=\"inf639\" -e WANDB_ENTITY=\"your-entity-name\" -v ./:/inf639 inf639:latest python3 ./your_dqn_training_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Hands on Atari environment\n",
    "\n",
    "The Atari environment is a popular benchmark for reinforcement learning algorithms. It consists of a collection of over 50 Atari games, each with a unique set of challenges and rewards. The goal is to train an agent to play these games at a human-level performance or better.\n",
    "\n",
    "The Atari environment is described [here](https://gymnasium.farama.org/environments/atari/).\n",
    "\n",
    "In this lab, we will use the `ALE/Breakout-v5` environment, where the agent controls a paddle to bounce a ball and break a wall of bricks. The agent receives a reward for each brick it breaks and loses a life when the ball passes the paddle.\n",
    "\n",
    "<img src=\"https://gymnasium.farama.org/_images/breakout.gif\"></img>\n",
    "\n",
    "This environment is described [here](https://gymnasium.farama.org/environments/atari/breakout/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some information about the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode=\"rgb_array\")\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n.item()\n",
    "\n",
    "print(f\"State space size is: { env.observation_space }\")\n",
    "print(f\"Action space size is: { env.action_space }\")\n",
    "print(\"Actions are: {\" + \", \".join([str(a) for a in range(env.action_space.n)]) + \"}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cells and check different basic \n",
    "policies (for instance randomly drawn actions) to discover the Breakout environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ALE/Breakout-v5', render_mode='rgb_array')\n",
    "RenderWrapper.register(env, force_gif=True)\n",
    "\n",
    "for episode_index in range(5):\n",
    "    observation, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(70):\n",
    "        env.render_wrapper.render()\n",
    "\n",
    "        if not done:\n",
    "            print(observation)\n",
    "        else:\n",
    "            print(\"x\", end=\"\")\n",
    "\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    print()\n",
    "    env.close()\n",
    "\n",
    "env.render_wrapper.make_gif(FIGS_DIR / \"lab1_ex8random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is proposed in this lab\n",
    "\n",
    "Training an Atari game model takes too long (around 4 hours with a recent GPU like an RTX 4060 or higher).\n",
    "A pre-trained model has been optimized and reset only the last two layers (dense layers).\n",
    "You now only need to retrain your model on these two specific layers.\n",
    "\n",
    "This makes training much faster, reducing it to about ten minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Download pre-trained model\n",
    "\n",
    "The pretrained model is available at the following URL: https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/refs/heads/main/models/dqn_atari_weights_randomized_layers_7_and_9.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/refs/heads/main/models/dqn_atari_weights_randomized_layers_7_and_9.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load the pre-trained weights and freeze the convolutional layers, you can use the following code snippet:\n",
    "\n",
    "```python\n",
    "# ...\n",
    "\n",
    "# Load pretrained weights if specified\n",
    "q_network.load_state_dict(torch.load(\"dqn_atari_weights_randomized_layers_7_and_9.pth\", map_location=device))\n",
    "\n",
    "# Freeze convolutional layers\n",
    "for param in q_network.network[:6].parameters():  # Assuming the first 6 layers are convolutional\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, q_network.parameters()), lr=args.learning_rate)\n",
    "\n",
    "# ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Model presentation\n",
    "\n",
    "In the original DQN paper, the authors used a deep convolutional neural network to approximate the Q-function. The network consists of three convolutional layers followed by two fully connected layers. The input to the network is a stack of four consecutive frames, which allows the agent to perceive motion.\n",
    "A ReLu activation function is used after each layer, except for the output layer, which uses a linear activation function.\n",
    "\n",
    "If you need a refresher on convolutional neural networks, check out the following resources: \n",
    "- https://towardsdatascience.com/conv2d-to-finally-understand-what-happens-in-the-forward-pass-1bbaafb0b148\n",
    "- https://github.com/AxelThevenot/GIF_convolutions\n",
    "\n",
    "<img src=\"https://github.com/AxelThevenot/GIF_convolutions/blob/master/GIFS/Input%20Shape%20:%20(3,%207,%207)%20-%20Output%20Shape%20:%20(2,%207,%207)%20-%20K%20:%20(3,%203)%20-%20P%20:%20(1,%201)%20-%20S%20:%20(1,%201)%20-%20D%20:%20(1,%201)%20-%20G%20:%201.gif?raw=true\" width=400></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU\n",
    "\n",
    "For CartPole, using a GPU was neither useful nor relevant because the model is too simple. The communication overhead between the CPU and GPU outweighed the speed gains in forward and backward pass computations.\n",
    "\n",
    "However, for the Atari environment, the situation is different. The model is more complex, and using a GPU can significantly speed up the computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrappers\n",
    "\n",
    "The atari environment requires some preprocessing to be used with a DQN agent. The following wrappers are recommended:\n",
    "- Stable Baselines3 Atari Wrappers: https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
    "- Gymnasium Atari Wrappers: https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.AtariPreprocessing\n",
    "\n",
    "In the following implementation to complete, Stable Baselines3 Atari Wrappers are used.\n",
    "\n",
    "[Machado paper](https://www.jair.org/index.php/jair/article/view/11182/26388) explains the importance of the preprocessing steps in the Atari environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network visualization with Zetane\n",
    "\n",
    "You can use [Zetane](https://github.com/zetane/viewer?tab=readme-ov-file#zetane-viewer) to visualize the network architecture. It provides an interactive way to explore the structure of neural networks, including the layers, connections, and parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "You can use the same hyperparameters than https://arxiv.org/abs/1312.5602 (DQN 2013) or https://arxiv.org/abs/1509.06461 (DDQN 2015) or https://arxiv.org/abs/1511.06581 (DDDQN 2015) or https://arxiv.org/abs/1511.05952 (PER 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement and train DQN on Breakout\n",
    "\n",
    "Complete and test the following code to implement and train a DQN agent on the Breakout environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import gymnasium as gym\n",
    "# import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from stable_baselines3.common.atari_wrappers import ClipRewardEnv, EpisodicLifeEnv, FireResetEnv, MaxAndSkipEnv, NoopResetEnv\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "import torch\n",
    "# import tqdm\n",
    "\n",
    "#DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = torch.nn.Sequential(\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "            \n",
    "            ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)\n",
    "\n",
    "\n",
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)\n",
    "\n",
    "\n",
    "def make_env():\n",
    "    #env = gym.make(\"ALE/Breakout-v5\", render_mode=\"rgb_array\")\n",
    "    env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "    # https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
    "    env = NoopResetEnv(env, noop_max=30)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = EpisodicLifeEnv(env)\n",
    "    env = FireResetEnv(env)\n",
    "    env = ClipRewardEnv(env)\n",
    "    env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "    env = gym.wrappers.GrayScaleObservation(env)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    return env\n",
    "\n",
    "\n",
    "def train(\n",
    "    num_training_timesteps: int = 3_000_000,\n",
    "    learning_rate: float = 1e-4,\n",
    "    buffer_size: int = 500_000,\n",
    "    gamma: float = 0.99,\n",
    "    batch_size: int = 32,\n",
    "    initial_epsilon_value: float = 1.0,\n",
    "    final_epsilon_value: float = 0.01,\n",
    "    exploration_fraction: float = 0.10,\n",
    "    initial_learning_timestep: int = 80_000,\n",
    "    train_frequency: int = 4,\n",
    "    target_network_frequency: int = 1_000,\n",
    "    tau: float = 1.0,\n",
    "    wandb_project_name: str = None,         # W&B's project name\n",
    "    wandb_entity: str = None                # W&B's entity (username or team name)\n",
    "):\n",
    "    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    run_name = f\"dqn_atari_v2015_sb3_wrappers_{current_time}\"\n",
    "\n",
    "    print(run_name)\n",
    "\n",
    "    # W&B setup\n",
    "    if wandb_project_name is None:\n",
    "        wandb_project_name = os.getenv('WANDB_PROJECT_NAME', None)\n",
    "\n",
    "    if wandb_entity is None:\n",
    "        wandb_entity = os.getenv('WANDB_ENTITY', None)\n",
    "\n",
    "    use_wandb = wandb_project_name is not None and wandb_entity is not None\n",
    "\n",
    "    if use_wandb:\n",
    "        import wandb\n",
    "\n",
    "        # wandb.login()\n",
    "\n",
    "        wandb.init(\n",
    "            project=wandb_project_name,\n",
    "            entity=wandb_entity,\n",
    "            # sync_tensorboard=True,\n",
    "            config={\n",
    "                \"num_training_timesteps\": num_training_timesteps,\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"buffer_size\": buffer_size,\n",
    "                \"gamma\": gamma,\n",
    "                \"batch_size\": batch_size,\n",
    "                \"initial_epsilon_value\": initial_epsilon_value,\n",
    "                \"final_epsilon_value\": final_epsilon_value,\n",
    "                \"exploration_fraction\": exploration_fraction,\n",
    "                \"initial_learning_timestep\": initial_learning_timestep,\n",
    "                \"train_frequency\": train_frequency,\n",
    "                \"target_network_frequency\": target_network_frequency,\n",
    "                \"tau\": tau\n",
    "            },\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "\n",
    "    # Create environment\n",
    "    env = make_env()\n",
    "\n",
    "    # Create networks\n",
    "    q_network = QNetwork(env).to(DEVICE)\n",
    "\n",
    "    # Load pretrained weights\n",
    "    q_network.load_state_dict(torch.load(\"dqn_atari_weights_randomized_layers_7_and_9.pth\", map_location=device))\n",
    "\n",
    "    # Freeze convolutional layers\n",
    "    for param in q_network.network[:6].parameters():  # Assuming the first 6 layers are convolutional\n",
    "        param.requires_grad = False\n",
    "\n",
    "    target_q_network = QNetwork(env).to(DEVICE)\n",
    "    target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "    # Create optimizer and loss function\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, q_network.parameters()), lr=learning_rate)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    # Create replay buffer\n",
    "    replay_buffer = ReplayBuffer(\n",
    "        buffer_size,\n",
    "        env.observation_space,\n",
    "        env.action_space,\n",
    "        DEVICE,\n",
    "        optimize_memory_usage=True,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "\n",
    "    training_step = 0\n",
    "    info = {\"lives\": 0}\n",
    "    while training_step < num_training_timesteps:\n",
    "        if info['lives'] == 0:\n",
    "            observation, info = env.reset()\n",
    "            episode_reward = 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            training_step += 1\n",
    "            # env.render()\n",
    "\n",
    "            # Take action with epsilon-greedy policy\n",
    "            epsilon = linear_schedule(initial_epsilon_value, final_epsilon_value, exploration_fraction * num_training_timesteps, training_step)\n",
    "            if random.random() < epsilon:\n",
    "                action = np.array(env.action_space.sample())\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    observation_batch_tensor = torch.Tensor(observation).unsqueeze(0).to(DEVICE)\n",
    "                    q_values_tensor = q_network(observation_batch_tensor).squeeze(0)\n",
    "                    action = torch.argmax(q_values_tensor).cpu().numpy()\n",
    "\n",
    "            # Step environment\n",
    "            next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                if info['lives'] == 0:\n",
    "                    print(f\"Training step: {training_step} ; Episode reward: {episode_reward}\")\n",
    "                    # print(f\"Lives: {info['lives']}\")\n",
    "                    if use_wandb:\n",
    "                        wandb.log({\n",
    "                            \"training_step\": training_step,\n",
    "                            \"episode_reward\": episode_reward,\n",
    "                            \"num_transitions_in_replay_buffer\": replay_buffer.size(),\n",
    "                            \"episode_frame_number\": info[\"episode_frame_number\"],\n",
    "                            \"frame_number\": info[\"frame_number\"],\n",
    "                            \"lives\": info[\"lives\"]\n",
    "                        })\n",
    "\n",
    "            # Add transition to replay buffer\n",
    "            replay_buffer.add(observation, next_observation, action, reward, terminated, info)\n",
    "\n",
    "            # Update the action value function\n",
    "            if training_step > initial_learning_timestep:\n",
    "                if training_step % train_frequency == 0:\n",
    "                    # Sample a batch of transitions\n",
    "\n",
    "                    ### BEGIN SOLUTION ###\n",
    "\n",
    "                    td_target_batch = ...\n",
    "                    q_value_batch = ...\n",
    "                    loss = ...\n",
    "\n",
    "                    ### END SOLUTION ###\n",
    "\n",
    "                    if use_wandb and (training_step % (train_frequency * 10) == 0):\n",
    "                        wandb.log({\n",
    "                            \"training_step\": training_step,\n",
    "                            \"loss\": loss,\n",
    "                            \"epsilon\": epsilon,\n",
    "                            \"q_value_min\":    q_value_batch.min().item(),\n",
    "                            \"q_value_max\":    q_value_batch.max().item(),\n",
    "                            \"q_value_mean\":   q_value_batch.mean().item(),\n",
    "                            \"q_value_median\": q_value_batch.median().item(),\n",
    "                            \"td_target_min\":    td_target_batch.min().item(),\n",
    "                            \"td_target_max\":    td_target_batch.max().item(),\n",
    "                            \"td_target_mean\":   td_target_batch.mean().item(),\n",
    "                            \"td_target_median\": td_target_batch.median().item()\n",
    "                        })\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # Update target network\n",
    "                if training_step % target_network_frequency == 0:\n",
    "                    target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            # Update the observation\n",
    "            observation = next_observation\n",
    "    \n",
    "    # Save the model\n",
    "\n",
    "    model_path = pathlib.Path(\"runs\") / f\"{run_name}_weights.pth\"\n",
    "    torch.save(q_network.state_dict(), model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Gradient clipping\n",
    "\n",
    "Gradient clipping is a technique used to tackle the problem of exploding gradients in deep learning, including in the context of Reinforcement Learning. In reinforcement learning, the distribution of rewards and therefore gradients can be highly variable. Large updates to the Q-network weights can destabilize training, leading to divergence. Gradient clipping limits the size of the weight updates, ensuring stable and more reliable learning. By preventing erratic and large updates, gradient clipping can help the learning process converge more smoothly and often more quickly to a stable policy. DQN uses experience replay to break the correlation between successive updates. However, the mixed nature of the experiences can lead to high variance in gradients. Clipping gradients in this context ensures that even if there is a harmful experience in the replay buffer, it does not disproportionately affect the learning process.\n",
    "\n",
    "Gradient clipping can be implemented using `torch.nn.utils.clip_grad_value_` between the `loss.backward()` and the `optimizer.step()` as follow:\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "torch.nn.utils.clip_grad_value_(q_network.parameters(), clip_grad_value)  # In-place gradient clipping\n",
    "optimizer.step()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further readings\n",
    "\n",
    "### Distributional methods\n",
    "\n",
    "Distributional Q-Learning enhances the standard DQN by modeling the entire distribution of possible future rewards, rather than simply estimating the expected reward (https://arxiv.org/abs/1707.06887). The motivation for Distributional Q-Learning stems from the insight that the uncertainty in rewards—and consequently in the value function—can provide valuable information that is not captured when only the mean expected reward is considered.\n",
    "\n",
    "Standard DQN approximates the expected value of the total return (the sum of future discounted rewards) from a given state-action pair. However, this approach ignores the variability around this expected value. In contrast, Distributional Q-Learning represents the value function as a distribution over possible returns, capturing the full range of potential outcomes and their probabilities.\n",
    "\n",
    "This richer representation allows the agent to distinguish between actions that may lead to the same expected reward but with different risks or variances in outcomes. It enables more informed decision-making in stochastic environments, where the variability of returns is as important as the expectation. By capturing the entire distribution, Distributional Q-Learning can also potentially converge faster and yield more robust policies, as it accounts for the variance in rewards that can be critical in the learning process.\n",
    "\n",
    "### NoisyNets DQN\n",
    "\n",
    "Noisy DQN introduces parametric noise directly into the weights of the neural network to drive exploration, as opposed to traditional methods like ϵϵ-greedy where the randomness is injected into the action selection process (https://arxiv.org/abs/1706.10295). The key motivation behind Noisy DQN is to enable more sophisticated and efficient exploration strategies.\n",
    "\n",
    "In standard DQN, exploration is often implemented through ϵϵ-greedy policies that select random actions with a certain probability ϵϵ, which can be suboptimal and inefficient, especially as ϵϵ has to be carefully decayed over time to balance exploration with exploitation. This randomness is external and does not adapt based on the agent’s experience.\n",
    "\n",
    "Noisy DQN, however, incorporates noise into the network’s parameters, making the policy itself stochastic. This approach allows the network to learn the degree of exploration needed from the environment feedback, as the noise can be learned and adapted during training. It provides a more nuanced exploration mechanism that can potentially learn to explore in a state-dependent manner, leading to faster and more robust convergence of the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rainbow paper: Putting everything together\n",
    "\n",
    "In  2017, Henssel et al. performed a large experiment that combined several DQN enhancements, among them DDQN and PER (https://arxiv.org/abs/1710.02298). They found that the ehancements worked well together. The paper has become known as the Rainbow paper, since the major graph showing the cumulative performance over 57 Atrari games is multicolored.\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/lab1_rainbow_curve.png?raw=true\" width=\"600px\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Value-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2024](https://moodle.polytechnique.fr/course/view.php?id=19358) Lab session #1\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of the most renowned value-based reinforcement learning techniques, specifically *Deep Q-Networks* and its enhancements.\n",
    "\n",
    "In this Python notebook, you will implement and evaluate *Deep Q-Networks* (DQN) and its various adaptations.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab1_deep_value-based_reinforcement_learning.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab1_deep_value-based_reinforcement_learning.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important note\n",
    "\n",
    "This tutorial has been tested with Python 3.10 and Python 3.11.\n",
    "\n",
    "It is important to note that **Bonus Section 4: *Test and train a DQN agent to play Atari games*** [is not compatible with Python 3.12](https://github.com/Farama-Foundation/Gymnasium/issues/1081) (the latest stable version of Python). If you plan to complete this bonus section locally on your machine rather than on Google Colab, make sure you have Python 3.10 or 3.11 installed.\n",
    "\n",
    "If you are using Python 3.12 and prefer not to run this notebook on Google Colab, an alternative solution via Docker will be provided at the beginning of Bonus Section 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following survey to help us improve this lab session: https://moodle.polytechnique.fr/mod/questionnaire/view.php?id=535310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### Deep value-based reinforcement learning\n",
    "\n",
    "Deep reinforcement learning methods like DQN (Deep Q-Networks) are significant advancements over tabular methods such as Q-Learning because they can handle complex, high-dimensional environments that were previously intractable. While Q-Learning is limited to environments where the state and action spaces are sufficiently small to maintain a table of values, DQN uses neural networks to approximate the Q-value function, allowing it to generalize across similar states and scale to problems with vast state spaces. This enables DQN to learn optimal policies for tasks like video games, robotic control, and other applications where the number of possible states is extraordinarily large.\n",
    "\n",
    "While DQN was designed to tackle large environments like Atari games, the primary focus of this lab is to delve into the underlying algorithms, understand them thoroughly, and evaluate them comprehensively. It's important to note that working with not-so-deep networks captures the essence of deep reinforcement learning, excluding the computational expense. The transition from tabular Q-learning to DQN involves significant implications, primarily due to the ability of DQN to handle high-dimensional state spaces. Moving from DQN to very-deep-DQN is primarily a matter of scale and computational resources. The core principles remain the same, and understanding these principles is the key to mastering reinforcement learning, regardless of the complexity of the network used.\n",
    "For these reasons, in this lab, we will focus on studying the CartPole environment. The CartPole problem is a classic in reinforcement learning, and it provides a simpler and more manageable context for understanding the principles of DQN. The convergence in the CartPole environment is much faster than in Atari games - typically within a minute, as opposed to approximately hours on a well-equipped personal computer for Atari games. This allows us to experiment and iterate more quickly, facilitating a deeper understanding of the algorithms at play.\n",
    "\n",
    "We will therefore focus on algorithms in a simple environment. However, once the key concepts are mastered and tested in these settings, Bonus Section 4 will offer you the opportunity to refine your technical skills by applying them to the game Breakout in the Atari environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook relies on several libraries including `torch`, `gymnasium`, `numpy`, `pandas`, `seaborn`, `imageio`, `pygame`, and `tqdm`.\n",
    "A complete list of dependencies can be found in the provided [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) and [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) files.\n",
    "\n",
    "- [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains the minimal dependencies required to run this notebook without the optional sections (e.g. the Atari environment, Aim, Tensorboard, Optuna, Weights&Bias, ...).\n",
    "- [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) contains all the dependencies required to run this notebook with all the optional sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you use Google Colab\n",
    "\n",
    "If you use Google Colab, execute the next cell to install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xt66Z85AmOI2",
    "outputId": "bd7a6b75-ad3c-4be1-d560-8239fbc0e9d2"
   },
   "outputs": [],
   "source": [
    "import sys, subprocess\n",
    "\n",
    "def is_colab():\n",
    "    return \"google.colab\" in sys.modules\n",
    "\n",
    "def run_subprocess_command(cmd):\n",
    "    # run the command\n",
    "    process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE)\n",
    "    # print the output\n",
    "    for line in process.stdout:\n",
    "        print(line.decode().strip())\n",
    "\n",
    "if is_colab():\n",
    "    run_subprocess_command(\"apt install xvfb x11-utils\")\n",
    "    run_subprocess_command(\"pip install -r https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-google-colab.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJfFvm-4mOI2"
   },
   "outputs": [],
   "source": [
    "#! apt install xvfb x11-utils && pip install gymnasium pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you have downloaded the notebook on your computer and execute it in your own Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up the necessary dependencies, first download the [requirements.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements.txt) or [requirements-minimal.txt](https://raw.githubusercontent.com/jeremiedecock/polytechnique-inf639-2024-students/main/requirements-minimal.txt) depending on whether you want to run the optional sections of this notebook or not (c.f. *Setup the Python environment* section above).\n",
    "\n",
    "Ensure it is located in the same directory as this notebook. Next, run the following command to establish a [Python virtual environment (venv)](https://docs.python.org/3/library/venv.html) that includes all the essential libraries for this lab.\n",
    "\n",
    "#### On Posix systems (Linux, MacOSX, WSL, ...)\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version.\n",
    "\n",
    "#### On Windows\n",
    "\n",
    "```bash\n",
    "python3 -m venv env\n",
    "env\\Scripts\\activate.bat\n",
    "python3 -m pip install --upgrade pip\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Adapt the name of the requirements file if you have chosen to use the minimal version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Run notebooks locally in a dedicated Docker container\n",
    "\n",
    "If you are familiar with Docker, an image is available on Docker Hub for this lab:\n",
    "\n",
    "```bash\n",
    "docker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work jdhp/inf639-lab1:latest\n",
    "```\n",
    "\n",
    "If you encounter an error during the notebook's execution indicating that writing a file is not possible, this issue may stem from the user ID within the container lacking the necessary permissions in the project directory. This problem can be resolved by modifying the directory's permissions, for example, using the command:\n",
    "\n",
    "```bash\n",
    "chmod 777 . figs models\n",
    "rm -rf figs/*.gif\n",
    "rm -rf figs/*.png\n",
    "rm -rf models/*.pth\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gymnasium as gym\n",
    "import itertools\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Tuple, Deque, Optional, Callable\n",
    "# from inf639 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "MODELS_DIR = Path(\"models/\")   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir()\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Gymnasium rendering wrapper to visualize environments as GIF images within the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook allows you to visualize the episodes as animated GIFs. Run the cell below to enable this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAFnXFZcrr-9"
   },
   "outputs": [],
   "source": [
    "# To display GIF images in the notebook\n",
    "\n",
    "import imageio     # To render episodes in GIF images (otherwise there would be no render on Google Colab)\n",
    "                   # C.f. https://stable-baselines.readthedocs.io/en/master/guide/examples.html#bonus-make-a-gif-of-a-trained-agent\n",
    "import IPython\n",
    "from IPython.display import Image\n",
    "\n",
    "if is_colab():\n",
    "    import pyvirtualdisplay\n",
    "\n",
    "    _display = pyvirtualdisplay.Display(visible=False,  # use False with Xvfb\n",
    "                                        size=(1400, 900))\n",
    "    _ = _display.start()\n",
    "\n",
    "class RenderWrapper:\n",
    "    def __init__(self, env, force_gif=False):\n",
    "        self.env = env\n",
    "        self.force_gif = force_gif\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.images = []\n",
    "\n",
    "    def render(self):\n",
    "        if not is_colab():\n",
    "            self.env.render()\n",
    "            time.sleep(1./60.)\n",
    "\n",
    "        if is_colab() or self.force_gif:\n",
    "            img = self.env.render()         # Assumes env.render_mode == 'rgb_array'\n",
    "            self.images.append(img)\n",
    "\n",
    "    def make_gif(self, filename=\"render\"):\n",
    "        if is_colab() or self.force_gif:\n",
    "            gif_path = filename.with_suffix('.gif')\n",
    "            imageio.mimsave(gif_path, [np.array(img) for i, img in enumerate(self.images) if i%2 == 0], fps=29, loop=0)\n",
    "            return Image(open(gif_path,'rb').read())\n",
    "\n",
    "    @classmethod\n",
    "    def register(cls, env, force_gif=False):\n",
    "        env.render_wrapper = cls(env, force_gif=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define some parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of trainings\n",
    "\n",
    "To achieve more representative outcomes at the conclusion of each exercise, we average the results across multiple training sessions. The `NUMBER_OF_TRAININGS` variable specifies the number of training sessions conducted before the results are displayed. \n",
    "\n",
    "We recommend setting a lower value (such as 1 or 2) during the development and testing phases of your implementations. Once you have completed your work and are confident in its functionality, you can increase the number of training sessions to minimize the variance in results. Be aware that a higher number of training sessions will extend the execution time, so adjust this setting in accordance with your computer's capabilities.\n",
    "\n",
    "Additionally, you have the option to assign a specific value to the `NUMBER_OF_TRAININGS` variable for each exercise directly within the cells where the training loop is defined (the `NUMBER_OF_TRAININGS` variable is commented out at the beginning of these cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_NUMBER_OF_TRAININGS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Refresher and Cheat Sheet\n",
    "\n",
    "In this lab, we will be implementing our deep reinforcement learning algorithms using PyTorch.\n",
    "If you need a refresher, you might find this [PyTorch Cheat Sheet](https://pytorch.org/tutorials/beginner/ptcheat.html) helpful. It provides a quick reference for many of the most commonly used PyTorch functions and concepts, and can be a valuable resource as you work through this lab.\n",
    "\n",
    "You can also refer to the [official documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch can run on both CPUs and GPUs. The following cell will determine the device PyTorch will use. If a GPU is available, PyTorch will use it; otherwise, it will use the CPU.\n",
    "\n",
    "For utilizing a GPU on Google Colab, you also have to activate it following the steps outlined [here](https://colab.research.google.com/notebooks/gpu.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the GPU is not very useful for CartPole (but useful for Atari Breakout in *Bonus section 4*) because CartPole is a simple and quick problem to solve, and CUDA spends more time transferring data between the CPU and GPU than processing it directly on the CPU.\n",
    "\n",
    "You can uncomment the next cell to explicitly instruct PyTorch to train neural networks using the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"PyTorch will train and test neural networks on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a recent GPU (e.g. RTX 4060 Ti 16G) and want to use it, you may need to install a specific version of PyTorch compatible with your Cuda version (e.g. Cuda 12.4). For this, you will have to edit the `requirements.txt` file and replace the current version of PyTorch with the one compatible with your Cuda version. Check the [official PyTorch website](https://pytorch.org/get-started/locally/) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus section 1: Monitoring the training process with Tensorboard and Aim\n",
    "\n",
    "Monitoring the training process is crucial for understanding the agent's learning progress. In this section, we will introduce two open-source tools that can help you visualize the training process: Tensorboard and Aim.\n",
    "\n",
    "A third tool, [Weights & Biases (W&B)](https://wandb.ai/site) will be presented in *Bonus section 3*.\n",
    "\n",
    "Tensorflow and Aim are particularly useful for monitoring the training process on a local machine, as they do not require a server to run.\n",
    "On the opposite, W&B is a cloud-based solution that allows you to monitor the training process from anywhere.\n",
    "Other solutions will be covered in the next tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with Tensorboard\n",
    "\n",
    "TensorBoard is a visualization tool developed by Google, primarily used to track and visualize metrics during the training of deep learning models. While initially designed for TensorFlow, it can also be integrated with **PyTorch** through the **torch.utils.tensorboard** API. This allows real-time tracking of metrics such as losses, accuracy, computation graphs, and even examining images, weight distributions, histograms, and more.\n",
    "\n",
    "The following cell demonstrates how to use TensorBoard with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Simple example: model and data\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Creating a model, optimizer, and loss function\n",
    "model = SimpleModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Initializing TensorBoard SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Training example\n",
    "for epoch in range(100):\n",
    "    # Fake data\n",
    "    inputs = torch.randn(32, 10)\n",
    "    targets = torch.randn(32, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Log the loss values to TensorBoard\n",
    "    writer.add_scalar('Loss/train', loss.item(), epoch)\n",
    "\n",
    "# Closing SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`SummaryWriter`** is the central tool for logging data to TensorBoard. It captures metric values, like loss, for each iteration/epoch.\n",
    "\n",
    "This example trains a simple model on fake data (32 samples with 10 features) over 100 epochs, and at each epoch, the loss value is sent to TensorBoard via `writer.add_scalar`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=runs\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:6006/).\n",
    "\n",
    "If you are using Google Colab, you can use the **TensorBoard magic** to visualize the data directly in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TensorBoard extension in Colab\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For mor information, check the Official Documentation:\n",
    "- [TensorBoard for PyTorch Documentation](https://pytorch.org/docs/stable/tensorboard.html)\n",
    "- [Official TensorBoard Documentation (General)](https://www.tensorflow.org/tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring the training process with AIM\n",
    "\n",
    "Aim is an open-source alternative to TensorBoard for managing and visualizing machine learning experiments.\n",
    "Designed to be simpler and more flexible, **Aim** allows tracking and analyzing metrics, hyperparameters, model outputs, and more for projects using PyTorch, TensorFlow, or other frameworks.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/13848158/136374529-af267918-5dc6-4a4e-8ed2-f6333a332f96.gif\" width=\"600px\"></img>\n",
    "\n",
    "The following cell demonstrates how to use Aim with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import aim\n",
    "\n",
    "# Initialize Aim\n",
    "run = aim.Run()\n",
    "\n",
    "# set training hyperparameters\n",
    "run['hparams'] = {\n",
    "    'learning_rate': 0.01,\n",
    "    'batch_size': 32,\n",
    "}\n",
    "\n",
    "# Simple model example\n",
    "model = torch.nn.Linear(1, 1)\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10):\n",
    "    inputs = torch.randn(10, 1)\n",
    "    targets = torch.randn(10, 1)\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Track the 'loss' metric\n",
    "    run.track(loss.item(), name='loss', epoch=epoch)    # Log metric with Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example trains a simple model on fake data over 10 epochs, and at each epoch, the loss value is sent to Aim via `run.track`.\n",
    "\n",
    "To visualize the data from a local machine, run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "aim up\n",
    "```\n",
    "\n",
    "then open a web browser and go at the address mentioned in the terminal (usually http://localhost:43800/).\n",
    "\n",
    "Or alternatively, you can use the **Aim magic** to [visualize the data directly in the notebook](https://aimstack.readthedocs.io/en/latest/using/jupyter_notebook_ui.html) (for instance if you are using Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext aim\n",
    "%aim up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information, check the official Documentation:\n",
    "- [Aim Documentation](https://aimstack.readthedocs.io/en/latest/)\n",
    "- [GitHub Aim](https://github.com/aimhubio/aim)\n",
    "- [Quick start](https://github.com/aimhubio/aim#-quick-start)\n",
    "- [Getting started](https://aimstack.readthedocs.io/en/latest/quick_start/setup.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-based Reinforcement Learning\n",
    "\n",
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/blob/main/assets/logo.jpg?raw=true\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "[CSC_53439_EP-2024](https://moodle.polytechnique.fr/course/view.php?id=19358) Lab session #3\n",
    "\n",
    "2019-2024 Jérémie Decock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab3_mcts.ipynb)\n",
    "\n",
    "[![My Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab3_mcts.ipynb)\n",
    "\n",
    "[![NbViewer](https://raw.githubusercontent.com/jupyter/design/master/logos/Badges/nbviewer_badge.svg)](https://nbviewer.jupyter.org/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab3_mcts.ipynb)\n",
    "\n",
    "[![Local](https://img.shields.io/badge/Local-Save%20As...-blue)](https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab3_mcts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In our preceding labs, we've delved into model-free approaches, encompassing both value-based and policy-based methods.\n",
    "\n",
    "Model-free reinforcement learning methods learn directly from episodes of experience. They are called \"model-free\" because they do not need to know or learn the model of the environment (i.e., the reward and transition probability functions). Examples include Q-learning and policy gradients.\n",
    "\n",
    "On the other hand, model-based reinforcement learning methods attempt to first learn a model of the environment (i.e., the reward and transition probability functions), and then use this model to make decisions. They can plan ahead by simulating future states in the model, which can lead to more efficient learning than model-free methods.\n",
    "\n",
    "The aim of this lab is to provide an in-depth exploration of one of the most famous model-based reinforcement learning method: *Monte Carlo Tree Search (MCTS)*.\n",
    "We will focus on the planning aspect of model-based reinforcement learning (using MCTS), so we provide an internal model (*NaughtsAndCrossesState* and *NaughtsAndCrossesAction*) but in many practical cases, the model is not available and must be learned from experience.\n",
    "\n",
    "In this Python notebook, you will implement and assess this algorithm on the Naughts and Crosses board game.\n",
    "\n",
    "You can either:\n",
    "- open, edit and execute the notebook in *Google Colab* following this link: https://colab.research.google.com/github/jeremiedecock/polytechnique-inf639-2024-students/blob/main/lab3_mcts.ipynb ; this is the **recommended** choice as you have nothing to install on your computer\n",
    "- open, edit and execute the notebook in *MyBinder* (if for any reason the Google Colab solution doesn't work): https://mybinder.org/v2/gh/jeremiedecock/polytechnique-inf639-2024-students/main?filepath=lab3_mcts.ipynb\n",
    "- download, edit and execute the notebook on your computer if Python3 and JypyterLab are already installed: https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/main/lab3_mcts.ipynb\n",
    "\n",
    "If you work with Google Colab or MyBinder, **remember to save or download your work regularly or you may lose it!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Python environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook relies on the graphviz tool.\n",
    "\n",
    "If you are using Google Colab, graphviz is already installed.\n",
    "Otherwise, check https://graphviz.org/.\n",
    "\n",
    "For instance, on Debian Gnu/Linux, Ubuntu, ..., you can install it with the following command:\n",
    "```bash\n",
    "apt install graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! apt install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from copy import deepcopy\n",
    "from functools import reduce\n",
    "import math\n",
    "import operator\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import Optional, Any, List, Union, Callable\n",
    "\n",
    "from IPython.display import Image   # To display graphviz images in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGS_DIR = Path(\"figs/\")       # Where to save figures (.gif files)\n",
    "PLOTS_DIR = Path(\"figs/\")      # Where to save plots (.png or .svg files)\n",
    "MODELS_DIR = Path(\"models/\")   # Where to save models (.pth files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FIGS_DIR.exists():\n",
    "    FIGS_DIR.mkdir()\n",
    "if not PLOTS_DIR.exists():\n",
    "    PLOTS_DIR.mkdir()\n",
    "if not MODELS_DIR.exists():\n",
    "    MODELS_DIR.mkdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Monte Carlo Tree Search for Naughts and Crosses (Tic-Tac-Toe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MCTS algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Tree Search (MCTS) is a model-based reinforcement learning algorithm that uses Monte Carlo simulation to evaluate actions in a decision-making problem. It's particularly effective in complex domains such as games, where it's not feasible to compute an exact solution.\n",
    "\n",
    "Every time a decision is required, the MCTS algorithm performs the following steps for a specified number of simulations:\n",
    "1. **Selection**: Start from the current root node and select successive child nodes down to a leaf node. The section path is determined by a *tree policy*.\n",
    "2. **Expansion**: If the leaf node is not a terminal node (i.e., it does not end the game), then create one or more child nodes and select one.\n",
    "3. **Simulation**: Run a simulated play of the game, either to the end of the game or a predetermined depth limit.\n",
    "4. **Backpropagation**: Use the result of the game simulation to update information in the nodes on the path from the root to the leaf node.\n",
    "\n",
    "The action leading to the child node with the highest value is then chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/jeremiedecock/polytechnique-inf639-2024-students/raw/refs/heads/main/assets/mcts.png\" />\n",
    "\n",
    "(source: Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a pseudo-code version of the algorithm:\n",
    "\n",
    "```\n",
    "function MCTS(root):\n",
    "    while within computational budget:\n",
    "        leaf = selectNode(root)\n",
    "        simulation_result = simulate(leaf)\n",
    "        backpropagate(leaf, simulation_result)\n",
    "    return bestChild(root)\n",
    "\n",
    "function selectNode(node):\n",
    "    while node is not a leaf:\n",
    "        if not all children of node have been expanded:\n",
    "            return expand(node)\n",
    "        else:\n",
    "            node = bestUCT(node)  # UCT = Upper Confidence bound applied to Trees\n",
    "    return node\n",
    "\n",
    "function expand(node):\n",
    "    choose an unexpanded child of node\n",
    "    create a new child node\n",
    "    return the new child node\n",
    "\n",
    "function simulate(node):\n",
    "    simulate a random playout from node\n",
    "    return the result of the playout\n",
    "\n",
    "function backpropagate(node, result):\n",
    "    while node is not null:\n",
    "        update the node with the result\n",
    "        node = node's parent\n",
    "\n",
    "function bestChild(node):\n",
    "    return child of node with the highest win ratio\n",
    "```\n",
    "\n",
    "Here's a brief explanation of each part:\n",
    "\n",
    "- **MCTS Function**: It runs the main loop of the algorithm. Within the computational budget (like a time limit or a maximum number of iterations), it selects nodes, simulates games from those nodes, and then backpropagates the results.\n",
    "\n",
    "- **Select Node**: This function traverses the tree from the root to a leaf node by using a selection policy like UCT (Upper Confidence bound applied to Trees).\n",
    "\n",
    "- **Expand**: If the node has any untried actions, it creates a new child node for one of these actions.\n",
    "\n",
    "- **Simulate**: From the new node, it simulates a random game (or a game with some heuristic-based decisions), often until a terminal state of the game is reached.\n",
    "\n",
    "- **Backpropagate**: Once the simulation is complete, the results (like win/loss) are propagated back up the tree, updating the statistics (like win rate) of each node along the path from the leaf node to the root.\n",
    "\n",
    "- **Best Child**: When the computational budget is exhausted, the child of the root with the highest win ratio (or another metric) is chosen as the move to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "\n",
    "- Kocsis, Levente, and Csaba Szepesvári. \"Bandit based monte-carlo planning.\" In European conference on machine learning, pp. 282-293. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006. https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6661e57237e4e8739b7a4946c4d3d4875376c068\n",
    "\n",
    "- Browne, Cameron B., Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. \"A survey of monte carlo tree search methods.\" IEEE Transactions on Computational Intelligence and AI in games 4, no. 1 (2012): 1-43. https://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf\n",
    "\n",
    "- Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", sect. 8.11, p.185 (http://incompleteideas.net/book/RLbook2020.pdf).\n",
    "\n",
    "- Kamil Czarnogòrski, \"Monte Carlo Tree Search – beginners guide\", 2018 https://int8.io/monte-carlo-tree-search-beginners-guide/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Playout policy and simulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by implementing the rollout (aka playout) policy, also known as the rollout or default policy, which is utilized during the simulation phase (the third phase of the MCTS algorithm).\n",
    "\n",
    "The subsequent two cells establish the Action and State classes for the [Naughts and Crosses environment](https://en.wikipedia.org/wiki/Tic-tac-toe) (also known as Tic-tac-toe). For this lab, we'll forgo the use of the Gym/Gymnasium framework in favor of a custom, tailored environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaughtsAndCrossesAction():\n",
    "    \"\"\"\n",
    "    Represents an action in the game of Naughts and Crosses (Tic-Tac-Toe).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    player : int\n",
    "        The player making the action. 1 for 'X', -1 for 'O'.\n",
    "    x : int\n",
    "        The x-coordinate of the action on the game board.\n",
    "    y : int\n",
    "        The y-coordinate of the action on the game board.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, player: int, x: int, y: int):\n",
    "        self.player = player\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return str((self.x, self.y))\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return str(self)\n",
    "\n",
    "    def __eq__(self, other: Any) -> bool:\n",
    "        return self.__class__ == other.__class__ and self.x == other.x and self.y == other.y and self.player == other.player\n",
    "\n",
    "    def __hash__(self) -> int:\n",
    "        return hash((self.x, self.y, self.player))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaughtsAndCrossesState():\n",
    "    \"\"\"\n",
    "    Represents the state of the game of Naughts and Crosses (Tic-Tac-Toe).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    PLAYER_SYMBOLS : dict of int to str\n",
    "        A dictionary mapping the player numerical values used internally to their symbols ('X' or 'O').\n",
    "    board : list of list of int\n",
    "        The game board represented as a 2D list.\n",
    "    current_player : int\n",
    "        The current player making a move. 1 for 'X', -1 for 'O'.\n",
    "    \"\"\"\n",
    "\n",
    "    PLAYER_SYMBOLS = {\n",
    "        1: \"X\",\n",
    "        -1: \"O\"\n",
    "    }\n",
    "\n",
    "    def __init__(self, board: Optional[List[List[int]]] = None, current_player: Optional[int] = 1):\n",
    "        self.board: List[List[int]] = board if board is not None else [[0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "        self.current_player: int = current_player\n",
    "\n",
    "    def getCurrentPlayer(self) -> int:\n",
    "        \"\"\"\n",
    "        Get the current player making a move.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "            The current player. 1 for 'X', -1 for 'O'.\n",
    "        \"\"\"\n",
    "        return self.current_player\n",
    "\n",
    "    def getPossibleActions(self) -> List[NaughtsAndCrossesAction]:\n",
    "        \"\"\"\n",
    "        Get a list of possible actions that the current player can take.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of NaughtsAndCrossesAction\n",
    "            A list of possible actions that the current player can take.\n",
    "        \"\"\"\n",
    "        possible_actions = []\n",
    "        for i in range(len(self.board)):\n",
    "            for j in range(len(self.board[i])):\n",
    "                if self.board[i][j] == 0:\n",
    "                    possible_actions.append(NaughtsAndCrossesAction(player=self.current_player, x=i, y=j))\n",
    "        return possible_actions\n",
    "\n",
    "    def takeAction(self, action: NaughtsAndCrossesAction) -> \"NaughtsAndCrossesState\":\n",
    "        \"\"\"\n",
    "        Take an action and return the resulting state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        action : NaughtsAndCrossesAction\n",
    "            The action to take.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NaughtsAndCrossesState\n",
    "            The resulting state after taking the action.\n",
    "        \"\"\"\n",
    "        new_state = deepcopy(self)\n",
    "        new_state.board[action.x][action.y] = action.player\n",
    "        new_state.current_player = self.current_player * -1\n",
    "        return new_state\n",
    "\n",
    "    def isTerminal(self) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the current state is a terminal state (game over).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        bool\n",
    "            True if the state is terminal (game over), False otherwise.\n",
    "        \"\"\"\n",
    "        for row in self.board:\n",
    "            if abs(sum(row)) == 3:\n",
    "                return True\n",
    "        for column in list(map(list, zip(*self.board))):\n",
    "            if abs(sum(column)) == 3:\n",
    "                return True\n",
    "        for diagonal in [[self.board[i][i] for i in range(len(self.board))],\n",
    "                         [self.board[i][len(self.board) - i - 1] for i in range(len(self.board))]]:\n",
    "            if abs(sum(diagonal)) == 3:\n",
    "                return True\n",
    "        return reduce(operator.mul, sum(self.board, []), 1)\n",
    "\n",
    "    def getReward(self) -> Union[int, bool]:\n",
    "        \"\"\"\n",
    "        Get the reward for the current state.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Union[int, bool]\n",
    "            The reward value if the state is terminal, False otherwise.\n",
    "        \"\"\"\n",
    "        for row in self.board:\n",
    "            if abs(sum(row)) == 3:\n",
    "                return sum(row) / 3\n",
    "        for column in list(map(list, zip(*self.board))):\n",
    "            if abs(sum(column)) == 3:\n",
    "                return sum(column) / 3\n",
    "        for diagonal in [[self.board[i][i] for i in range(len(self.board))],\n",
    "                         [self.board[i][len(self.board) - i - 1] for i in range(len(self.board))]]:\n",
    "            if abs(sum(diagonal)) == 3:\n",
    "                return sum(diagonal) / 3\n",
    "        return False\n",
    "    \n",
    "    def graphviz_str(self) -> str:\n",
    "        output_str = \"\"\n",
    "        for row in self.board:\n",
    "            output_str += ''.join([NaughtsAndCrossesState.PLAYER_SYMBOLS[x].lower() if x in (-1, 1) else '_' for x in row]) + '\\n'\n",
    "        return output_str\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        output_str = f\"Current player: {NaughtsAndCrossesState.PLAYER_SYMBOLS[self.current_player]}\\n\"\n",
    "        for row in self.board:\n",
    "            output_str += ' '.join([NaughtsAndCrossesState.PLAYER_SYMBOLS[x] if x in (-1, 1) else '_' for x in row]) + '\\n'\n",
    "        return output_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Monte Carlo Tree Search (MCTS), the `random_policy` is used during the simulation phase of the algorithm. It's a type of rollout policy where actions are selected randomly from the set of available actions in each state.\n",
    "\n",
    "The purpose of this rollout policy is to provide an estimate of the value of a state by simulating the outcome of the game from that state to the end, under the assumption that both players play randomly.\n",
    "This estimate is then used to update the value of the state in the MCTS tree. It guides the selection of actions in future simulations by the *tree policy* during the selection phase (step 1) of MCTS.\n",
    "\n",
    "**Task 1.1**: implement the `random_policy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_policy(state: NaughtsAndCrossesState) -> NaughtsAndCrossesAction:\n",
    "    \"\"\"\n",
    "    Select a random action from the possible actions in the current state.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : NaughtsAndCrossesState\n",
    "        The current state of the game.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    NaughtsAndCrossesAction\n",
    "        A randomly selected action from the possible actions in the current state.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "    action = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use this rollout policy in the simulate function.\n",
    "\n",
    "The `simulate` function in Monte Carlo Tree Search (MCTS) is used to perform the simulation phase of the algorithm. \n",
    "\n",
    "In this phase, the algorithm plays out a complete game or a certain number of steps from the current state, according to a specified policy (in this case, `rollout_policy`). The policy is typically a simple, fast-to-compute policy, such as choosing actions uniformly at random.\n",
    "\n",
    "The purpose of the simulation is to obtain an estimate of the value of the current state. This estimate is then used to update the values of the states and actions in the MCTS tree, which guides the selection of actions in future simulations.\n",
    "\n",
    "The `simulate` function has to continue to select actions and transition to new states until it reaches a terminal state (a state that ends the game). It then has to return the reward of the terminal state, which serves as the estimate of the value of the initial state.\n",
    "\n",
    "**Task 1.2**: implement the `simulate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(state: NaughtsAndCrossesState, rollout_policy: Callable[[NaughtsAndCrossesState], NaughtsAndCrossesAction], verbose: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Simulate a random game from the current state and return the reward value.\n",
    "\n",
    "        This function serves as a random policy for the Monte Carlo Tree Search (MCTS) algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : NaughtsAndCrossesState\n",
    "            The current state of the game.\n",
    "        rollout_policy : Callable[[NaughtsAndCrossesState], float]\n",
    "            The policy for simulating random games from a state.\n",
    "        verbose : bool\n",
    "            Print all traversed states and taken actions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The reward value after simulating a random game from the current state.\n",
    "        \"\"\"\n",
    "        while not state.isTerminal():\n",
    "\n",
    "            ### BEGIN SOLUTION ###\n",
    "\n",
    "            action = ...\n",
    "            state = ...\n",
    "\n",
    "            ### END SOLUTION ###\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"State: {state}\")\n",
    "                print(f\"Action: {action}\")\n",
    "                print()\n",
    "\n",
    "        return state.getReward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell to check if your implementation works as intended.\n",
    "\n",
    "**Task 1.3**: test the `random_policy` and the `simulate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = NaughtsAndCrossesState()\n",
    "\n",
    "reward = simulate(state, random_policy, verbose=True)\n",
    "\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: the \"expand\" step\n",
    "\n",
    "We will implement the `expand` function, but first, we define the `TreeNode` class. The `TreeNode` class is a data structure that represents a node in the MCTS tree.\n",
    "In this deterministic environment, each node represents a state in the problem's state space, and each edge represents an action that transitions between states.\n",
    "\n",
    "Here are some typical attributes of a `TreeNode`:\n",
    "\n",
    "- `state`: The state in the state space that this node represents.\n",
    "- `parent`: The parent node of this node in the MCTS tree. The root node has a `parent` of `None`.\n",
    "- `children`: A dictionary mapping actions to child nodes. If an action leads from this node's state to another state, then `children[action]` is the `TreeNode` for that state.\n",
    "- `is_fully_expanded`: A boolean flag indicating whether all possible actions from this node's state have been added to the `children` dictionary.\n",
    "- `num_visits`: The number of times this node has been visited during the MCTS algorithm.\n",
    "- `total_reward`: The total reward received during simulations that passed through this node. This is used to compute the average reward of this node, which is `total_reward / num_visits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    \"\"\"\n",
    "    Represents a node in the Monte Carlo Tree Search (MCTS) algorithm.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    state : NaughtsAndCrossesState\n",
    "        The state of the game at this node.\n",
    "    is_terminal : bool\n",
    "        True if the state is a terminal state, False otherwise.\n",
    "    is_fully_expanded : bool\n",
    "        True if all possible actions have been expanded, False otherwise.\n",
    "    parent : TreeNode\n",
    "        The parent node of this node.\n",
    "    num_visits : int\n",
    "        The number of times this node has been visited.\n",
    "    total_reward : float\n",
    "        The total reward accumulated at this node.\n",
    "    children : dict of NaughtsAndCrossesAction to TreeNode\n",
    "        A dictionary mapping actions to child nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state: NaughtsAndCrossesState,\n",
    "        parent: Optional[\"TreeNode\"]\n",
    "    ):\n",
    "        self.state = state\n",
    "        self.is_terminal = state.isTerminal()\n",
    "        self.is_fully_expanded = self.is_terminal\n",
    "        self.parent = parent\n",
    "        self.num_visits = 0\n",
    "        self.total_reward = 0\n",
    "        self.children = {}\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        s = []\n",
    "        s.append(f\"total_reward: {self.total_reward}\")\n",
    "        s.append(f\"num_visits: {self.num_visits}\")\n",
    "        s.append(f\"is_terminal: {self.is_terminal}\")\n",
    "        s.append(f\"possible_actions: {list(self.children.keys())}\")\n",
    "        return f\"{self.__class__.__name__}: \" + ', '.join(s)\n",
    "\n",
    "    def to_graphiz(\n",
    "        self,\n",
    "        save_file_path: Path = FIGS_DIR / \"mcts.dot\",\n",
    "        exploration_ratio: float = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Plots the tree with the current node as the root.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        save_file_path : str, optional\n",
    "            the path to save the plot (default is None, which means the plot will not be saved)\n",
    "        \"\"\"\n",
    "\n",
    "        dot_str = \"digraph G {\\n\"\n",
    "        dot_str += \"    node [shape=box];\\n\"\n",
    "\n",
    "        # Build the graph and add the current node as the root.\n",
    "        # Recursively adds nodes and edges to the graph `G`, assigns labels to nodes, and assigns colors to nodes.\n",
    "        queue = deque([self])\n",
    "\n",
    "        while len(queue) > 0:\n",
    "            # Retrive the last element\n",
    "            node = queue.popleft()\n",
    "\n",
    "            if node.parent is not None:\n",
    "                # Add the edge from the parent node to the current node\n",
    "                parent_state_str = node.parent.state.graphviz_str()\n",
    "                state_str = node.state.graphviz_str()\n",
    "                if exploration_ratio is not None:\n",
    "                    ucb_score_str = f\"/{ucb(node, exploration_ratio)}\"\n",
    "                else:\n",
    "                    ucb_score_str = \"\"\n",
    "                dot_str += f'    \"{node.parent.total_reward}/{node.parent.num_visits}{ucb_score_str}\\n{parent_state_str}\" -> \"{node.total_reward}/{node.num_visits}\\n{state_str}\"\\n'\n",
    "\n",
    "            # Add child node into the queue\n",
    "            queue.extend(node.children.values())\n",
    "\n",
    "        dot_str += \"}\\n\"\n",
    "\n",
    "        with open(save_file_path, \"w\") as f:\n",
    "            f.write(dot_str)\n",
    "\n",
    "        return dot_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `expand` function in Monte Carlo Tree Search (MCTS) is used to perform the expansion phase of the algorithm.\n",
    "\n",
    "In this phase, the algorithm chooses a leaf node from the current MCTS tree (i.e., a node that has not been expanded yet), and creates one or more child nodes corresponding to the actions that can be taken from the state represented by the leaf node. One of these child nodes is then selected for the simulation phase.\n",
    "\n",
    "The purpose of the expansion phase is to grow the MCTS tree by exploring new states of the game. This allows the algorithm to gradually build up a more and more accurate representation of the value of each state and action, as more simulations are performed.\n",
    "\n",
    "In the `expand` function, you should creates a new child node for each possible action from the current state, and adds these child nodes to the MCTS tree. You should then select one of these child nodes (typically at random) for the simulation phase.\n",
    "\n",
    "**Task 2.1**: Implement the `expand` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand(node: TreeNode) -> TreeNode:\n",
    "    \"\"\"\n",
    "    Expand the given node by adding a child node for a randomly selected unexplored action.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : TreeNode\n",
    "        The node to expand.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TreeNode\n",
    "        The newly added child node.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    actions = ...\n",
    "    for action in actions:\n",
    "        if action not in node.children:\n",
    "            new_node = ...\n",
    "            ...\n",
    "            return new_node\n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell to check if your implementation works as intended.\n",
    "\n",
    "**Task 2.2**: test the `expand` and `TreeNode.to_graphviz` functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = NaughtsAndCrossesState(board=[[0, 0, 0], [-1, 1, 1], [-1, 1, -1]], current_player=1)\n",
    "root_node = TreeNode(root_state, None)\n",
    "\n",
    "new_node1 = expand(root_node)\n",
    "new_node2 = expand(root_node)\n",
    "new_node3 = expand(root_node)\n",
    "\n",
    "new_node11 = expand(new_node1)\n",
    "new_node12 = expand(new_node1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: the \"backpropogate\" step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `backpropagate` function in Monte Carlo Tree Search is used to perform the backpropagation phase of the algorithm.\n",
    "\n",
    "In this phase, after a simulation has been performed and a reward has been obtained, the algorithm updates the information in the nodes on the path from the root to the leaf node that was expanded. This typically involves incrementing the `num_visits` attribute of each node on the path, and adding the reward to the `total_reward` attribute of each node.\n",
    "\n",
    "The purpose of the backpropagation phase is to propagate the information obtained from the simulation back up the tree, so that it can guide the selection of actions in future simulations.\n",
    "\n",
    "In your implementation, the `backpropagate` function has take as input a path of nodes and the reward obtained from the simulation, and updates the `num_visits` and `total_reward` attributes of each node on the path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3.1**: implement the `backpropogate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogate(node: TreeNode, reward: float):\n",
    "    \"\"\"\n",
    "    Backpropagate the reward value from a leaf node up to the root node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : TreeNode\n",
    "        The leaf node where the simulation ended.\n",
    "    reward : float\n",
    "        The reward value obtained from the simulation.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    ...\n",
    "\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell to check if your implementation works as intended.\n",
    "\n",
    "**Task 3.2**: test the `backpropogate` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = NaughtsAndCrossesState(board=[[0, 0, 0], [-1, 1, 1], [0, 0, -1]], current_player=1)\n",
    "root_node = TreeNode(root_state, None)\n",
    "\n",
    "new_node1 = expand(root_node)\n",
    "new_node2 = expand(root_node)\n",
    "new_node3 = expand(root_node)\n",
    "new_node4 = expand(root_node)\n",
    "new_node5 = expand(root_node)\n",
    "\n",
    "new_node11 = expand(new_node1)\n",
    "new_node21 = expand(new_node2)\n",
    "new_node22 = expand(new_node2)\n",
    "new_node31 = expand(new_node3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "backpropogate(new_node1, 0.)\n",
    "backpropogate(new_node2, 1.)\n",
    "backpropogate(new_node3, 1.)\n",
    "backpropogate(new_node4, 0.)\n",
    "backpropogate(new_node5, 0.)\n",
    "\n",
    "backpropogate(new_node11, 0.)\n",
    "backpropogate(new_node21, 1.)\n",
    "backpropogate(new_node22, 0.)\n",
    "backpropogate(new_node31, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: The \"tree policy\" and the UCB\n",
    "\n",
    "The Upper Confidence Bound (UCB) formula is used during the selection phase (the first step) of Monte Carlo Tree Search. \n",
    "In this phase, starting from the root node, the algorithm selects the best child node according to a certain criterion, and continues this process until it reaches a leaf node. The criterion for selecting the best child is typically the Upper Confidence Bound (UCB) formula, which balances the exploitation of child nodes with high average reward and the exploration of child nodes with few visits.\n",
    "\n",
    "The USB formula is usually written as follows\n",
    "\n",
    "$$\n",
    "\\text{UCB} = \\frac{w_a}{n_a} + c \\sqrt{\\frac{\\ln n}{n_a}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_a$​ is the number of wins in child $a$.\n",
    "- $n_a$​ is the number of times chile $a$ has been visited.\n",
    "- $n$​ is the number of times the parent node has been visited.\n",
    "- $c$ is the exploration parameter, which is a constant determining the balance between exploration and exploitation. In practice, this is often set to a value that provides a good balance in the specific context (like $\\sqrt{2}$ in many board games).\n",
    "\n",
    "This formula balances two aspects: the exploitation of known good moves (represented by $\\frac{w_a}{n_a}​​$), and the exploration of less-tried moves (represented by $c \\sqrt{\\frac{\\ln⁡ n}{n_a}}$​). The constant $c$ adjusts how much the algorithm favors exploration over exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4.1**: Implement the `ucb` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(node: TreeNode, exploration_ratio: float) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the Upper Confidence Bound (UCB) for a node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parent_node : TreeNode\n",
    "        The parent node.\n",
    "    node : TreeNode\n",
    "        The node for which the UCB is calculated.\n",
    "    exploration_ratio : float\n",
    "        The exploration ratio for balancing exploration and exploitation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The calculated UCB value.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \n",
    "    node_value = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return node_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cell to check if your implementation works as intended.\n",
    "\n",
    "**Task 4.2**: test the ucb function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = NaughtsAndCrossesState(board=[[0, 0, 0], [-1, 1, 1], [0, 0, -1]], current_player=1)\n",
    "root_node = TreeNode(root_state, None)\n",
    "\n",
    "new_node1 = expand(root_node)\n",
    "new_node2 = expand(root_node)\n",
    "new_node3 = expand(root_node)\n",
    "new_node4 = expand(root_node)\n",
    "new_node5 = expand(root_node)\n",
    "\n",
    "new_node11 = expand(new_node1)\n",
    "new_node21 = expand(new_node2)\n",
    "new_node22 = expand(new_node2)\n",
    "new_node31 = expand(new_node3)\n",
    "\n",
    "backpropogate(new_node1, 0.)\n",
    "backpropogate(new_node2, 1.)\n",
    "backpropogate(new_node3, 1.)\n",
    "backpropogate(new_node4, 0.)\n",
    "backpropogate(new_node5, 0.)\n",
    "\n",
    "backpropogate(new_node11, 0.)\n",
    "backpropogate(new_node21, 1.)\n",
    "backpropogate(new_node22, 0.)\n",
    "backpropogate(new_node31, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb(new_node1, exploration_ratio=1/math.sqrt(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb(new_node2, exploration_ratio=1/math.sqrt(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the UCB formula, the `get_best_child` function is used during the selection phase of the algorithm.\n",
    "The `get_best_child` function computes the UCB value for each child of the input node, and returns the child with the highest UCB value.\n",
    "\n",
    "**Task 4.3**: implement the `get_best_child` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_child(node: TreeNode, exploration_ratio: float) -> TreeNode:\n",
    "    \"\"\"\n",
    "    Get the best child node based on the Upper Confidence Bound (UCB) formula.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : TreeNode\n",
    "        The parent node.\n",
    "    exploration_ratio : float\n",
    "        The exploration ratio for balancing exploration and exploitation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TreeNode\n",
    "        The best child node.\n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    ...\n",
    "    best_node = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return best_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cells to check if your implementation works as intended.\n",
    "\n",
    "**Task 4.4**: test the `get_best_child`  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = NaughtsAndCrossesState(board=[[0, 0, 0], [-1, 1, 1], [0, 0, -1]], current_player=1)\n",
    "root_node = TreeNode(root_state, None)\n",
    "\n",
    "new_node1 = expand(root_node)\n",
    "new_node2 = expand(root_node)\n",
    "new_node3 = expand(root_node)\n",
    "new_node4 = expand(root_node)\n",
    "new_node5 = expand(root_node)\n",
    "\n",
    "new_node11 = expand(new_node1)\n",
    "new_node21 = expand(new_node2)\n",
    "new_node22 = expand(new_node2)\n",
    "new_node31 = expand(new_node3)\n",
    "\n",
    "backpropogate(new_node1, 0.)\n",
    "backpropogate(new_node2, 1.)\n",
    "backpropogate(new_node3, 1.)\n",
    "backpropogate(new_node4, 0.)\n",
    "backpropogate(new_node5, 0.)\n",
    "\n",
    "backpropogate(new_node11, 0.)\n",
    "backpropogate(new_node21, 1.)\n",
    "backpropogate(new_node22, 0.)\n",
    "backpropogate(new_node31, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_node = get_best_child(root_node, exploration_ratio=1/math.sqrt(2))\n",
    "print(best_node)\n",
    "print(best_node.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `select_node` function in Monte Carlo Tree Search (MCTS) is the main function used to perform the selection phase of the algorithm.\n",
    "\n",
    "The `select_node` function takes the root node as input, and returns the leaf node selected according to the UCB criterion. It also returns the path of nodes from the root to the selected leaf node. This path is used in the backpropagation phase to update the information in the nodes.\n",
    "\n",
    "The `select_node` function iteratively selects the best child of the current node using the `get_best_child` function, until it reaches a leaf node. \n",
    "\n",
    "**Task 4.5**: implement the `select_node` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_node(node: TreeNode, exploration_ratio: float) -> TreeNode:\n",
    "    \"\"\"\n",
    "    Select a node in the tree for expansion using the Upper Confidence Bound (UCB) formula.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node : TreeNode\n",
    "        The current node in the tree.\n",
    "    exploration_ratio : float\n",
    "        The exploration ratio for selecting actions in the tree traversal phase.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    TreeNode\n",
    "        The selected node for expansion.\n",
    "    \"\"\"\n",
    "\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    ...\n",
    "    node = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    return node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cells to check if your implementation works as intended.\n",
    "\n",
    "**Task 4.6**: test the `select_node`  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_state = NaughtsAndCrossesState(board=[[0, 0, 0], [-1, 1, 1], [0, 0, -1]], current_player=1)\n",
    "root_node = TreeNode(root_state, None)\n",
    "\n",
    "new_node1 = expand(root_node)\n",
    "new_node2 = expand(root_node)\n",
    "new_node3 = expand(root_node)\n",
    "new_node4 = expand(root_node)\n",
    "new_node5 = expand(root_node)\n",
    "\n",
    "new_node11 = expand(new_node1)\n",
    "new_node21 = expand(new_node2)\n",
    "new_node22 = expand(new_node2)\n",
    "new_node31 = expand(new_node3)\n",
    "\n",
    "backpropogate(new_node1, 0.)\n",
    "backpropogate(new_node2, 1.)\n",
    "backpropogate(new_node3, 1.)\n",
    "backpropogate(new_node4, 0.)\n",
    "backpropogate(new_node5, 0.)\n",
    "\n",
    "backpropogate(new_node11, 0.)\n",
    "backpropogate(new_node21, 1.)\n",
    "backpropogate(new_node22, 0.)\n",
    "backpropogate(new_node31, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node.to_graphiz(save_file_path=FIGS_DIR / \"lab3_mcts_test.dot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_test.dot > {FIGS_DIR}/lab3_mcts_test.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_node = select_node(root_node, exploration_ratio=1/math.sqrt(2))\n",
    "print(selected_node)\n",
    "print(selected_node.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Putting all together in the search function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's integrate all the components within the `search` method of the MCTS class.\n",
    "\n",
    "**Task 5.1**: implement the `MCTS.search` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"\n",
    "    Monte Carlo Tree Search (MCTS) algorithm for the game of Naughts and Crosses (Tic-Tac-Toe).\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    exploration_ratio : float\n",
    "        The exploration ratio for selecting actions in the tree traversal phase.\n",
    "    rollout_policy : Callable[[NaughtsAndCrossesState], float]\n",
    "        The policy for simulating random games from a state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 exploration_ratio: float = 1/math.sqrt(2),\n",
    "                 rollout_policy: Callable[[NaughtsAndCrossesState], NaughtsAndCrossesAction] = random_policy):\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.rollout_policy = rollout_policy\n",
    "\n",
    "\n",
    "    def search(self, current_state: NaughtsAndCrossesState, num_simulations: int = 1000) -> NaughtsAndCrossesAction:\n",
    "        \"\"\"\n",
    "        Perform the Monte Carlo Tree Search (MCTS) algorithm to select the best action to take for the given state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_state : NaughtsAndCrossesState\n",
    "            The current state of the game.\n",
    "        num_simulations : int\n",
    "            The number of simulations to perform.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NaughtsAndCrossesAction\n",
    "            The best action to take according to the MCTS algorithm.\n",
    "        \"\"\"\n",
    "        self.root = TreeNode(current_state, None)\n",
    "\n",
    "        ### BEGIN SOLUTION ###\n",
    "\n",
    "        ...\n",
    "\n",
    "        ### END SOLUTION ###\n",
    "\n",
    "        best_child = get_best_child(self.root, 0)\n",
    "        action = (action for action, node in self.root.children.items() if node is best_child).__next__()\n",
    "\n",
    "        #print(\"expectedReward\", bestChild.total_reward / bestChild.num_visits)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now execute the following cells to check if your implementation works as intended.\n",
    "\n",
    "**Task 5.2**: Test the MCTS algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = NaughtsAndCrossesState()\n",
    "\n",
    "mcts = MCTS()\n",
    "\n",
    "i = 0\n",
    "while not state.isTerminal():\n",
    "    ### BEGIN SOLUTION ###\n",
    "\n",
    "    action = ...\n",
    "    state = ...\n",
    "\n",
    "    ### END SOLUTION ###\n",
    "\n",
    "    print(f\"State: {state}\")\n",
    "    print(f\"Action: {action}\")\n",
    "    print()\n",
    "    \n",
    "    mcts.root.to_graphiz(save_file_path=FIGS_DIR / f\"lab3_mcts_{i}.dot\")\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the MCTS Tree at some iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_4.dot > {FIGS_DIR}/lab3_mcts_4.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_5.dot > {FIGS_DIR}/lab3_mcts_5.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_5.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_6.dot > {FIGS_DIR}/lab3_mcts_6.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_6.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dot -Tpng {FIGS_DIR}/lab3_mcts_7.dot > {FIGS_DIR}/lab3_mcts_7.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(FIGS_DIR / 'lab3_mcts_7.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5.3**: change the `exploration_ratio` and watch the effect of this change on the graph. What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kocsis, Levente, and Csaba Szepesvári. \"Bandit based monte-carlo planning.\" In European conference on machine learning, pp. 282-293. Berlin, Heidelberg: Springer Berlin Heidelberg, 2006. https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=6661e57237e4e8739b7a4946c4d3d4875376c068\n",
    "\n",
    "- Browne, Cameron B., Edward Powley, Daniel Whitehouse, Simon M. Lucas, Peter I. Cowling, Philipp Rohlfshagen, Stephen Tavener, Diego Perez, Spyridon Samothrakis, and Simon Colton. \"A survey of monte carlo tree search methods.\" IEEE Transactions on Computational Intelligence and AI in games 4, no. 1 (2012): 1-43. https://repository.essex.ac.uk/4117/1/MCTS-Survey.pdf\n",
    "\n",
    "- Multi-armed Bandits: Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", chap.2 (http://incompleteideas.net/book/RLbook2020.pdf).\n",
    "\n",
    "- Introduction to MCTS: Richard S. Sutton and Andrew G. Barto: \"Reinforcement Learning: An Introduction\", sect. 8.11, p.185 (http://incompleteideas.net/book/RLbook2020.pdf).\n",
    "\n",
    "- Kamil Czarnogòrski, \"Monte Carlo Tree Search – beginners guide\", 2018 https://int8.io/monte-carlo-tree-search-beginners-guide/\n",
    "\n",
    "- Auer, Peter, Nicolo Cesa-Bianchi, and Paul Fischer. \"Finite-time analysis of the multiarmed bandit problem.\" Machine learning 47 (2002): 235-256. https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf\n",
    "\n",
    "- Couëtoux, Adrien, Jean-Baptiste Hoock, Nataliya Sokolovska, Olivier Teytaud, and Nicolas Bonnard. \"Continuous upper confidence trees.\" In Learning and Intelligent Optimization: 5th International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers 5, pp. 433-445. Springer Berlin Heidelberg, 2011. https://hal.science/hal-00542673/file/c0mcts.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
